# MoneyMaking_Crawler v3.2 - ë‹¨ìˆœí™” ê²€ìƒ‰ (í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œë§Œ!)
import os
import requests
import json
import random
import tempfile
import re
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, quote_plus
from io import BytesIO
import base64

from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from google.cloud import storage, translate_v3
from google.oauth2 import service_account
from langdetect import detect
from PIL import Image, ImageEnhance, ImageFilter
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseUpload
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_google_credentials():
    try:
        service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT')
        if service_account_json:
            service_account_info = json.loads(service_account_json)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials
    except Exception as e:
        print(f"Google ì¸ì¦ ì˜¤ë¥˜: {e}")
    return None

credentials = get_google_credentials()
if credentials:
    translate_client = translate_v3.TranslationServiceClient(credentials=credentials)
    storage_client = storage.Client(credentials=credentials)
    try:
        drive_service = build('drive', 'v3', credentials=credentials)
        docs_service = build('docs', 'v1', credentials=credentials)
    except:
        drive_service = None
        docs_service = None
else:
    translate_client = None
    storage_client = None
    drive_service = None
    docs_service = None

# 10ê°œêµ­ Google ë„ë©”ì¸ ë° ì–¸ì–´ ì½”ë“œ
TARGET_COUNTRIES = {
    'japan': {'domain': 'google.co.jp', 'lang': 'ja', 'translate_to': 'ja'},
    'germany': {'domain': 'google.de', 'lang': 'de', 'translate_to': 'de'},
    'france': {'domain': 'google.fr', 'lang': 'fr', 'translate_to': 'fr'},
    'italy': {'domain': 'google.it', 'lang': 'it', 'translate_to': 'it'},
    'spain': {'domain': 'google.es', 'lang': 'es', 'translate_to': 'es'},
    'netherlands': {'domain': 'google.nl', 'lang': 'nl', 'translate_to': 'nl'},
    'sweden': {'domain': 'google.se', 'lang': 'sv', 'translate_to': 'sv'},
    'norway': {'domain': 'google.no', 'lang': 'no', 'translate_to': 'no'},
    'denmark': {'domain': 'google.dk', 'lang': 'da', 'translate_to': 'da'},
    'austria': {'domain': 'google.at', 'lang': 'de', 'translate_to': 'de'}
}

# ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ í‚¤ì›Œë“œ (í™•ì¥)
PERSONAL_BLOG_INDICATORS = [
    'blog', 'diary', 'travel', 'journey', 'experience', 'visit', 'trip',
    'my', 'personal', 'life', 'adventure', 'story', 'log', 'went', 'been',
    'vacation', 'holiday', 'backpack', 'solo', 'couple', 'family',
    'review', 'guide', 'tips', 'recommendation', 'amazing', 'beautiful',
    'incredible', 'awesome', 'wonderful', 'memories', 'discover', 'explore'
]

# ê¸°ì—…/ì—¬í–‰ì‚¬ ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨
CORPORATE_EXCLUSIONS = [
    # ì˜ˆì•½ ì‚¬ì´íŠ¸
    'booking.com', 'agoda.com', 'expedia.com', 'hotels.com', 'airbnb.com',
    'kayak.com', 'priceline.com', 'orbitz.com', 'travelocity.com',
    'cheaptickets.com', 'momondo.com', 'skyscanner.com', 'trivago.com',
    
    # íˆ¬ì–´/ì•¡í‹°ë¹„í‹°
    'viator.com', 'getyourguide.com', 'klook.com', 'tiqets.com',
    'civitatis.com', 'attractiontix.com',
    
    # ì—¬í–‰ í¬í„¸/ë§¤ê±°ì§„
    'tripadvisor.com', 'lonelyplanet.com', 'roughguides.com', 'fodors.com',
    'frommers.com', 'ricksteves.com', 'culturetrip.com', 'planetware.com',
    'tripsavvy.com', 'travelandleisure.com', 'cntraveler.com', 'timeout.com',
    
    # ìœ„í‚¤/ì •ë³´
    'wikipedia.org', 'wikitravel.org', 'wikivoyage.org',
    
    # ì •ë¶€/ê³µì‹
    'gov', 'official', 'tourism', 'visit', 'destination',
    
    # ê¸°íƒ€
    'yelp.com', 'facebook.com', 'instagram.com', 'youtube.com'
]

def translate_keyword(keyword, target_lang):
    """í‚¤ì›Œë“œë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­"""
    if not translate_client or not credentials:
        return keyword
    
    try:
        parent = f"projects/{credentials.project_id}/locations/global"
        response = translate_client.translate_text(
            request={
                "parent": parent,
                "contents": [keyword],
                "mime_type": "text/plain",
                "source_language_code": "en",
                "target_language_code": target_lang,
            }
        )
        translated = response.translations[0].translated_text
        print(f"í‚¤ì›Œë“œ ë²ˆì—­: {keyword} â†’ {translated} ({target_lang})")
        return translated
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return keyword

def is_personal_blog(url, title, description):
    """ê°œì¸ ë¸”ë¡œê·¸ì¸ì§€ íŒë³„ (ì™„í™”ëœ ê¸°ì¤€)"""
    url_lower = url.lower()
    title_lower = title.lower() if title else ""
    desc_lower = description.lower() if description else ""
    
    # ê¸°ì—… ì‚¬ì´íŠ¸ ê°•ë ¥ ì œì™¸
    for exclusion in CORPORATE_EXCLUSIONS:
        if exclusion in url_lower:
            print(f"âŒ ê¸°ì—… ì‚¬ì´íŠ¸ ì œì™¸: {url[:50]}... (í¬í•¨: {exclusion})")
            return False
    
    # ê°œì¸ ë¸”ë¡œê·¸ ì§€í‘œ í™•ì¸
    text_to_check = f"{url_lower} {title_lower} {desc_lower}"
    personal_score = sum(1 for indicator in PERSONAL_BLOG_INDICATORS if indicator in text_to_check)
    
    # ê°œì¸ ë¸”ë¡œê·¸ í”Œë«í¼ íŒ¨í„´
    personal_patterns = [
        'wordpress.com', 'blogspot.com', 'blogger.com', 'medium.com',
        'tumblr.com', 'ghost.io', 'substack.com', 'wix.com', 'squarespace.com',
        '/blog/', '/travel/', '/diary/', '/journal/'
    ]
    
    pattern_score = sum(1 for pattern in personal_patterns if pattern in url_lower)
    
    total_score = personal_score + pattern_score
    is_personal = total_score >= 1  # 1ì  ì´ìƒì´ë©´ ê°œì¸ ë¸”ë¡œê·¸ë¡œ ì¸ì •
    
    print(f"{'âœ…' if is_personal else 'âŒ'} ë¸”ë¡œê·¸ íŒë³„: {url[:50]}... (ì ìˆ˜: {total_score})")
    return is_personal

def search_google_country(keyword, country_info, max_results=10):
    """íŠ¹ì • êµ­ê°€ì˜ Googleì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ë‹¨ìˆœí™”)"""
    
    # í‚¤ì›Œë“œë¥¼ í•´ë‹¹ êµ­ê°€ ì–¸ì–´ë¡œ ë²ˆì—­
    translated_keyword = translate_keyword(keyword, country_info['translate_to'])
    
    # ë‹¨ìˆœí•˜ê²Œ ë²ˆì—­ëœ í‚¤ì›Œë“œë§Œ ê²€ìƒ‰!
    search_query = translated_keyword  # ì¶”ê°€ ë‹¨ì–´ ì—†ì´ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ!
    
    encoded_query = quote_plus(search_query)
    search_url = f"https://www.{country_info['domain']}/search?q={encoded_query}&num={max_results}&hl={country_info['lang']}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': f"{country_info['lang']},en;q=0.9",
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }
    
    try:
        print(f"ğŸ” {country_info['domain']}ì—ì„œ ê²€ìƒ‰: {search_query}")
        response = requests.get(search_url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        results = []
        
        # Google ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        search_results = soup.find_all('div', class_='g') or soup.find_all('div', class_='tF2Cxc')
        
        for result in search_results[:max_results]:
            try:
                # ë§í¬ ì¶”ì¶œ
                link_elem = result.find('a', href=True)
                if not link_elem:
                    continue
                
                url = link_elem['href']
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                elif url.startswith('/search'):
                    continue
                
                if not url.startswith('http'):
                    continue
                
                # ì œëª© ì¶”ì¶œ
                title_elem = result.find('h3') or result.find('a')
                title = title_elem.get_text() if title_elem else ""
                
                # ì„¤ëª… ì¶”ì¶œ
                desc_candidates = result.find_all(['span', 'div'], class_=True)
                description = ""
                for candidate in desc_candidates:
                    text = candidate.get_text().strip()
                    if len(text) > 20 and len(text) < 200:
                        description = text
                        break
                
                # ê°œì¸ ë¸”ë¡œê·¸ íŒë³„
                if is_personal_blog(url, title, description):
                    results.append({
                        'url': url,
                        'title': title,
                        'description': description,
                        'country': country_info['domain'],
                        'language': country_info['lang'],
                        'search_query': search_query
                    })
                    print(f"âœ… ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬: {title[:50]}...")
                
            except Exception as e:
                print(f"ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ğŸ“Š {country_info['domain']}: {len(results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        return results
        
    except Exception as e:
        print(f"âŒ {country_info['domain']} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def process_4_3_image(img_data, keyword, index):
    """ì´ë¯¸ì§€ë¥¼ 4:3 ë¹„ìœ¨ë¡œ ì²˜ë¦¬í•˜ê³  ë³€ì¡°"""
    try:
        with Image.open(BytesIO(img_data)) as pil_img:
            # RGB ë³€í™˜
            if pil_img.mode != 'RGB':
                pil_img = pil_img.convert('RGB')
            
            width, height = pil_img.size
            
            # 4:3 ë¹„ìœ¨ë¡œ í¬ë¡­
            target_ratio = 4/3
            current_ratio = width/height
            
            if current_ratio > target_ratio:
                new_width = int(height * target_ratio)
                left = (width - new_width) // 2
                pil_img = pil_img.crop((left, 0, left + new_width, height))
            elif current_ratio < target_ratio:
                new_height = int(width / target_ratio)
                top = (height - new_height) // 2
                pil_img = pil_img.crop((0, top, width, top + new_height))
            
            # 800x600ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            pil_img = pil_img.resize((800, 600), Image.LANCZOS)
            
            # ì´ë¯¸ì§€ ë³€ì¡° (ì¶”ì  ë°©ì§€)
            pil_img = ImageEnhance.Brightness(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Contrast(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Color(pil_img).enhance(random.uniform(0.9, 1.1))
            
            # ì¢Œìš° ë°˜ì „ (50% í™•ë¥ )
            if random.choice([True, False]):
                pil_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)
            
            # ì•½ê°„ì˜ ë¸”ëŸ¬ íš¨ê³¼ (25% í™•ë¥ )
            if random.random() < 0.25:
                pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=0.5))
            
            temp_path = os.path.join(tempfile.gettempdir(), f"{keyword}_{index}_{random.randint(1000,9999)}.jpg")
            pil_img.save(temp_path, 'JPEG', quality=random.randint(85, 95), optimize=True)
            
            return temp_path
            
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def extract_blog_content(url, language):
    """ë¸”ë¡œê·¸ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_elements = soup.find_all(['p', 'div', 'article', 'section', 'main'])
        text_content = " ".join([elem.get_text(strip=True) for elem in text_elements])
        
        if len(text_content) < 300:
            return None
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = []
        img_tags = soup.find_all('img')
        
        for i, img in enumerate(img_tags[:20]):
            try:
                img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                if not img_url:
                    continue
                
                # URL ì •ê·œí™”
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    img_url = urljoin(url, img_url)
                elif not img_url.startswith('http'):
                    img_url = urljoin(url, img_url)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                img_response = requests.get(img_url, headers=headers, timeout=10)
                if img_response.status_code == 200:
                    img_data = img_response.content
                    
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    try:
                        with Image.open(BytesIO(img_data)) as test_img:
                            width, height = test_img.size
                            if width >= 150 and height >= 100:
                                processed_path = process_4_3_image(img_data, "image", len(images))
                                if processed_path:
                                    images.append(processed_path)
                    except:
                        continue
                        
            except Exception as e:
                continue
        
        # í…ìŠ¤íŠ¸ ë²ˆì—­
        if language != 'en':
            try:
                parent = f"projects/{credentials.project_id}/locations/global"
                response = translate_client.translate_text(
                    request={
                        "parent": parent,
                        "contents": [text_content[:2000]],
                        "mime_type": "text/plain",
                        "source_language_code": language,
                        "target_language_code": "en",
                    }
                )
                translated_text = response.translations[0].translated_text
            except:
                translated_text = text_content[:2000]
        else:
            translated_text = text_content[:2000]
        
        return {
            'url': url,
            'original_text': text_content[:2000],
            'translated_text': translated_text,
            'images': images,
            'language': language
        }
        
    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ì¶”ì¶œ ì˜¤ë¥˜ {url}: {e}")
        return None

def create_personal_story(blog_contents, keyword, location):
    """ì—¬ëŸ¬ ë¸”ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±"""
    
    if not blog_contents:
        return {"error": "ì²˜ë¦¬í•  ë¸”ë¡œê·¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    # ëª¨ë“  ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_texts = []
    for content in blog_contents:
        if content.get('translated_text'):
            all_texts.append(content['translated_text'])
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
    all_sentences = []
    for text in all_texts:
        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        all_sentences.extend(valid_sentences)
    
    random.shuffle(all_sentences)
    
    # ì£¼ì œë³„ ë¬¸ì¥ ë¶„ë¥˜
    intro_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['first', 'started', 'began', 'decided', 'planned'])]
    experience_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['amazing', 'beautiful', 'incredible', 'wonderful', 'stunning'])]
    place_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['place', 'location', 'area', 'spot', 'destination'])]
    food_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['food', 'eat', 'restaurant', 'delicious', 'taste'])]
    tip_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['tip', 'advice', 'recommend', 'suggest', 'important'])]
    
    # 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ êµ¬ì„±
    article = {
        "title": f"My Incredible {keyword.title()} Journey in {location}: A Complete Personal Experience Guide",
        
        "introduction": f"""
When I first planned my trip to {location} for {keyword}, I had high expectations, but nothing could have prepared me for the incredible journey that awaited me. After spending extensive time exploring this amazing destination, I'm excited to share my personal experience and insider tips that will help you make the most of your own {keyword} adventure in {location}.

This isn't just another generic travel guide - this is my authentic, first-hand account of {keyword} in {location}, filled with personal stories, unexpected discoveries, and practical advice that I wish I had known before my trip.
        """,
        
        "sections": [],
        
        "conclusion": f"""
Looking back on my {keyword} journey in {location}, I can honestly say it was one of the most transformative travel experiences of my life. Every single day brought new discoveries, incredible moments, and memories that I'll treasure forever.

{location} exceeded all my expectations for {keyword}, and I'm already planning my return trip. If you're considering {location} for your next {keyword} adventure, I can't recommend it highly enough.
        """,
        
        "word_count": 0
    }
    
    # 10ê°œ ì„¹ì…˜ ìƒì„±
    sections_data = [
        {
            "title": f"Why I Chose {location} for My {keyword.title()} Adventure",
            "sentences": intro_sentences[:5] + experience_sentences[:3],
            "intro": f"Let me start by sharing why {location} became my dream destination for {keyword}."
        },
        {
            "title": f"Planning My {keyword.title()} Trip to {location}",
            "sentences": tip_sentences[:4] + place_sentences[:4],
            "intro": f"Planning my {location} trip taught me valuable lessons."
        },
        {
            "title": f"First Impressions: Arriving in {location}",
            "sentences": experience_sentences[3:7] + place_sentences[4:7],
            "intro": f"The moment I stepped foot in {location}, I knew this trip would be special."
        },
        {
            "title": f"Top {keyword.title()} Experiences That Blew My Mind",
            "sentences": experience_sentences[7:12] + place_sentences[7:10],
            "intro": f"Some experiences in {location} were so incredible."
        },
        {
            "title": f"Hidden Gems I Discovered During My {location} Journey",
            "sentences": place_sentences[10:15] + experience_sentences[12:15],
            "intro": f"The best parts of my {location} adventure were the unexpected discoveries."
        },
        {
            "title": f"Food Adventures: My Culinary Journey in {location}",
            "sentences": food_sentences[:8] + experience_sentences[15:18],
            "intro": f"The food scene in {location} became an unexpected highlight."
        },
        {
            "title": f"Challenges I Faced and How I Overcame Them",
            "sentences": tip_sentences[4:8] + experience_sentences[18:21],
            "intro": f"Not everything went perfectly during my {location} trip."
        },
        {
            "title": f"Meeting Locals: The Heart of My {location} Experience",
            "sentences": experience_sentences[21:25] + place_sentences[15:18],
            "intro": f"The people I met in {location} made my journey truly unforgettable."
        },
        {
            "title": f"Budget Breakdown: What My {location} Trip Actually Cost",
            "sentences": tip_sentences[8:12] + experience_sentences[25:28],
            "intro": f"Here's the honest breakdown of what I spent."
        },
        {
            "title": f"Essential Tips for Your Own {location} Adventure",
            "sentences": tip_sentences[12:16] + place_sentences[18:22],
            "intro": f"Based on my experience, here are the most important things to know."
        }
    ]
    
    total_words = len(article["introduction"].split()) + len(article["conclusion"].split())
    
    # ê° ì„¹ì…˜ êµ¬ì„±
    for section_data in sections_data:
        if section_data["sentences"]:
            content = section_data["intro"] + " "
            
            # ë¬¸ì¥ë“¤ì„ ê°œì¸ ê²½í—˜ìœ¼ë¡œ ë³€í™˜
            personal_sentences = []
            for sentence in section_data["sentences"][:6]:
                personal_sentence = sentence
                personal_sentence = re.sub(r'\byou\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\byour\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\btheir\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\bthey\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentences.append(personal_sentence)
            
            full_content = content + " ".join(personal_sentences)
            
            article["sections"].append({
                "title": section_data["title"],
                "content": full_content
            })
            
            total_words += len(full_content.split())
    
    article["word_count"] = total_words
    return article

def create_word_document(article, all_images, keyword, location):
    """Word ë¬¸ì„œ ìƒì„± (ê¸€ + ì´ë¯¸ì§€ë“¤)"""
    
    doc = Document()
    
    # ì œëª©
    title = doc.add_heading(article['title'], 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ë©”íƒ€ ì •ë³´
    doc.add_paragraph(f"Word Count: {article['word_count']}")
    doc.add_paragraph(f"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"Keyword: {keyword}")
    doc.add_paragraph(f"Location: {location}")
    doc.add_paragraph()
    
    # ë„ì…ë¶€
    doc.add_heading("Introduction", 1)
    doc.add_paragraph(article['introduction'])
    
    # ë³¸ë¬¸ ì„¹ì…˜ë“¤
    for section in article['sections']:
        doc.add_heading(section['title'], 1)
        doc.add_paragraph(section['content'])
        doc.add_paragraph()
    
    # ê²°ë¡ 
    doc.add_heading("Conclusion", 1)
    doc.add_paragraph(article['conclusion'])
    
    # ì´ë¯¸ì§€ ì„¹ì…˜
    if all_images:
        doc.add_page_break()
        doc.add_heading("ğŸ“¸ Collected Images (4:3 Ratio)", 1)
        doc.add_paragraph("All images have been processed and optimized for your blog:")
        doc.add_paragraph()
        
        # ì´ë¯¸ì§€ë“¤ ì‚½ì…
        for i, img_path in enumerate(all_images, 1):
            try:
                if os.path.exists(img_path):
                    doc.add_paragraph(f"Image {i}:")
                    doc.add_picture(img_path, width=Inches(5.33))  # 4:3 ë¹„ìœ¨ ìœ ì§€
                    doc.add_paragraph()
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ì‚½ì… ì˜¤ë¥˜ {img_path}: {e}")
                continue
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_docx_path = os.path.join(tempfile.gettempdir(), f"MoneyMaking_{keyword}_{location}_{int(time.time())}.docx")
    doc.save(temp_docx_path)
    
    return temp_docx_path

def upload_to_google_drive(docx_path, keyword, location):
    """Google Driveì— Word ë¬¸ì„œ ì—…ë¡œë“œ"""
    
    if not drive_service:
        return {"success": False, "error": "Google Drive ì„œë¹„ìŠ¤ ì—†ìŒ"}
    
    try:
        # ë©”ì¸ í´ë” ìƒì„±
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        folder_name = f"MoneyMaking_{keyword}_{location}_{timestamp}"
        
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        
        folder = drive_service.files().create(body=folder_metadata).execute()
        folder_id = folder.get('id')
        
        # Word ë¬¸ì„œ ì—…ë¡œë“œ
        file_metadata = {
            'name': f"MoneyMaking_{keyword}_{location}_Complete_Article.docx",
            'parents': [folder_id]
        }
        
        media = MediaFileUpload(docx_path, mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        
        uploaded_file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,webViewLink'
        ).execute()
        
        folder_link = f"https://drive.google.com/drive/folders/{folder_id}"
        
        return {
            "success": True,
            "folder_link": folder_link,
            "document_link": uploaded_file.get('webViewLink'),
            "folder_name": folder_name
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

# Flask ë¼ìš°íŠ¸ë“¤
@app.route("/")
def home():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.2 - ë‹¨ìˆœí™” ê²€ìƒ‰ ì‹œìŠ¤í…œ",
        "status": "ğŸš€ DEPLOYED ON RAILWAY",
        "purpose": "10ê°œêµ­ ê°œì¸ ë¸”ë¡œê·¸ í¬ë¡¤ë§ â†’ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„± â†’ Word ë¬¸ì„œ ìë™ ì €ì¥",
        "features": [
            "ğŸŒ 10ê°œêµ­ Google ê²€ìƒ‰ (ê°œì¸ ë¸”ë¡œê·¸ë§Œ íƒ€ê²ŸíŒ…)",
            "ğŸ” í‚¤ì›Œë“œ ë‹¨ìˆœ ê²€ìƒ‰ (êµ¬ê¸€ì‹œíŠ¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ!)",
            "ğŸ“ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„±",
            "ğŸ–¼ï¸ ì´ë¯¸ì§€ 4:3 ë³€ì¡° ë° Word ì‚½ì…",
            "â˜ï¸ Google Drive ìë™ ì €ì¥",
            "ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨"
        ],
        "improvements_v32": [
            "âœ… í‚¤ì›Œë“œ ë‹¨ìˆœí™” - êµ¬ê¸€ì‹œíŠ¸ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ê²€ìƒ‰!",
            "âœ… ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì œê±° (4ê°œâ†’1ê°œ)",
            "âœ… ê²€ìƒ‰ íšŸìˆ˜ ëŒ€í­ ê°ì†Œ (40ë²ˆâ†’10ë²ˆ)",
            "âœ… Google ì°¨ë‹¨ ìœ„í—˜ ìµœì†Œí™”",
            "âœ… ì²˜ë¦¬ ì†ë„ í–¥ìƒ"
        ],
        "endpoints": {
            "home": "/",
            "test": "/test",
            "global_crawl": "/global_crawl",
            "quick_test": "/quick_test"
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/test")
def test():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.2 System Check",
        "google_cloud": "âœ… Connected" if credentials else "âŒ Not Connected",
        "google_drive": "âœ… Connected" if drive_service else "âŒ Not Connected",
        "translate_service": "âœ… Connected" if translate_client else "âŒ Not Connected",
        "target_countries": len(TARGET_COUNTRIES),
        "corporate_exclusions": len(CORPORATE_EXCLUSIONS),
        "personal_indicators": len(PERSONAL_BLOG_INDICATORS),
        "search_simplification": {
            "í‚¤ì›Œë“œ_ì²˜ë¦¬": "êµ¬ê¸€ì‹œíŠ¸ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©",
            "ê²€ìƒ‰_íšŸìˆ˜": "êµ­ê°€ë‹¹ 1ë²ˆ (ë‹¨ìˆœí™”)",
            "ì—¬í–‰ì‚¬ì´íŠ¸_ì°¨ë‹¨": f"{len(CORPORATE_EXCLUSIONS)}ê°œ ì‚¬ì´íŠ¸",
            "ê°œì¸ë¸”ë¡œê·¸_ê¸°ì¤€": "1ì  ì´ìƒ (ì™„í™”ë¨)"
        },
        "status": "ğŸš€ READY FOR SIMPLIFIED CRAWLING",
        "platform": "Railway",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/global_crawl", methods=['GET', 'POST'])
def global_crawl():
    """ğŸ’° ë‹¨ìˆœí™”ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    try:
        # íŒŒë¼ë¯¸í„° ë°›ê¸°
        if request.method == 'GET':
            keyword = request.args.get("keyword", "travel")
            location = request.args.get("location", "Europe")
            max_blogs = int(request.args.get("max_blogs", "3"))
        else:
            data = request.get_json() or {}
            keyword = data.get("keyword", "travel")
            location = data.get("location", "Europe")
            max_blogs = int(data.get("max_blogs", "3"))
        
        start_time = datetime.utcnow()
        print(f"ğŸ’° ë‹¨ìˆœí™”ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œì‘: {keyword} in {location}")
        print(f"ğŸ” í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ê²€ìƒ‰: '{keyword}'")
        
        # 1ë‹¨ê³„: 10ê°œêµ­ì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ë‹¨ìˆœí™”ëœ ë°©ì‹)
        print("ğŸŒ 1ë‹¨ê³„: ë‹¨ìˆœí™”ëœ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰...")
        all_blog_results = []
        
        countries_tried = 0
        for country_name, country_info in TARGET_COUNTRIES.items():
            if len(all_blog_results) >= max_blogs:  # ëª©í‘œ ë‹¬ì„±í•˜ë©´ ì¤‘ë‹¨
                break
                
            countries_tried += 1
            print(f"\nğŸ” {country_name} ê²€ìƒ‰ ì¤‘ ({countries_tried}/10)...")
            
            # ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ!)
            country_results = search_google_country(keyword, country_info, max_results=15)
            
            for result in country_results:
                if len(all_blog_results) >= max_blogs:
                    break
                all_blog_results.append(result)
            
            print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(all_blog_results)}/{max_blogs}ê°œ")
            time.sleep(random.uniform(3, 5))  # ë”œë ˆì´ ëŠ˜ë¦¼ (ì°¨ë‹¨ ë°©ì§€)
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if len(all_blog_results) >= max_blogs:
                print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±! {max_blogs}ê°œ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ")
                break
        
        if not all_blog_results:
            return {
                "success": False,
                "error": f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ê°œì¸ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "blog_search",
                "debug_info": {
                    "keyword_used": keyword,
                    "countries_tried": countries_tried,
                    "max_blogs_target": max_blogs,
                    "search_method": "ë‹¨ìˆœí™”ëœ í‚¤ì›Œë“œ ê²€ìƒ‰"
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(all_blog_results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        
        # 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© ë° ì´ë¯¸ì§€ í¬ë¡¤ë§
        print("\nğŸ“– 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§...")
        blog_contents = []
        all_images = []
        
        for i, blog_result in enumerate(all_blog_results):
            print(f"\ní¬ë¡¤ë§ ì¤‘ ({i+1}/{len(all_blog_results)}): {blog_result['title'][:50]}...")
            
            content = extract_blog_content(blog_result['url'], blog_result['language'])
            if content:
                blog_contents.append(content)
                all_images.extend(content['images'])
                print(f"âœ… ì„±ê³µ: {len(content['images'])}ê°œ ì´ë¯¸ì§€, {len(content['translated_text'])}ì")
            else:
                print(f"âŒ ì‹¤íŒ¨: {blog_result['url']}")
            
            time.sleep(random.uniform(2, 3))  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        if not blog_contents:
            return {
                "success": False,
                "error": "ë¸”ë¡œê·¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "content_extraction",
                "found_blogs": len(all_blog_results),
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(blog_contents)}ê°œ ë¸”ë¡œê·¸, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        
        # 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±
        print("\nâœï¸ 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ìƒì„±...")
        personal_article = create_personal_story(blog_contents, keyword, location)
        
        if "error" in personal_article:
            return {
                "success": False,
                "error": personal_article["error"],
                "stage": "story_creation",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {personal_article['word_count']}ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´")
        
        # 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±
        print("\nğŸ“„ 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±...")
        docx_path = create_word_document(personal_article, all_images, keyword, location)
        print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: Word ë¬¸ì„œ ìƒì„±")
        
        # 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ
        print("\nâ˜ï¸ 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ...")
        drive_result = upload_to_google_drive(docx_path, keyword, location)
        
        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        try:
            os.remove(docx_path)
            for img_path in all_images:
                if os.path.exists(img_path):
                    os.remove(img_path)
        except:
            pass
        
        # ìµœì¢… ê²°ê³¼
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        final_result = {
            "success": True,
            "global_crawling_info": {
                "ğŸ’° í‚¤ì›Œë“œ": keyword,
                "ğŸŒ íƒ€ê²Ÿ ìœ„ì¹˜": location,
                "â±ï¸ ì²˜ë¦¬ ì‹œê°„": f"{processing_time:.1f}ì´ˆ",
                "ğŸ” ê²€ìƒ‰ëœ ë¸”ë¡œê·¸": len(all_blog_results),
                "ğŸ“– ì„±ê³µì  í¬ë¡¤ë§": len(blog_contents),
                "ğŸ–¼ï¸ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€": len(all_images),
                "ğŸ“ ìµœì¢… ë‹¨ì–´ ìˆ˜": personal_article['word_count'],
                "ğŸ¯ ìƒíƒœ": "v3.2 ë‹¨ìˆœí™” ê²€ìƒ‰ìœ¼ë¡œ ì™„ë£Œ",
                "ğŸ” ê²€ìƒ‰ ë°©ì‹": "í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ê²€ìƒ‰"
            },
            
            "simplification_benefits": {
                "í‚¤ì›Œë“œ_ì²˜ë¦¬": f"'{keyword}' ê·¸ëŒ€ë¡œ ê²€ìƒ‰",
                "ê²€ìƒ‰_íšŸìˆ˜": f"ì´ {countries_tried}ê°œêµ­ì—ì„œ ê° 1ë²ˆì”©",
                "ì°¨ë‹¨_ìœ„í—˜": "ìµœì†Œí™”ë¨ (ë‹¨ìˆœ ê²€ìƒ‰)",
                "ì²˜ë¦¬_ì†ë„": "ë¹¨ë¼ì§ (ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ ì œê±°)"
            },
            
            "blog_sources": [
                {
                    "title": result['title'],
                    "url": result['url'],
                    "country": result['country'],
                    "language": result['language'],
                    "search_query": result.get('search_query', '')
                } for result in all_blog_results
            ],
            
            "article_preview": {
                "title": personal_article['title'],
                "word_count": personal_article['word_count'],
                "sections_count": len(personal_article['sections']),
                "introduction_preview": personal_article['introduction'][:200] + "...",
                "conclusion_preview": personal_article['conclusion'][:200] + "..."
            },
            
            "image_collection": {
                "total_processed": len(all_images),
                "format": "4:3 ratio (800x600)",
                "modifications": ["Brightness/Contrast adjusted", "50% chance horizontal flip", "Metadata removed"],
                "ready_for_blog": "âœ… Yes"
            },
            
            "google_drive_delivery": drive_result,
            
            "monetization_guide": {
                "1ë‹¨ê³„": "Google Driveì—ì„œ ì™„ì„±ëœ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                "2ë‹¨ê³„": "WordPressì— ê¸€ ë‚´ìš© ë³µì‚¬",
                "3ë‹¨ê³„": "ë¬¸ì„œ í•˜ë‹¨ì˜ ì´ë¯¸ì§€ë“¤ì„ ê¸€ ì¤‘ê°„ì¤‘ê°„ ì‚½ì…",
                "4ë‹¨ê³„": "ì–´í•„ë¦¬ì—ì´íŠ¸ ë§í¬ ë° ìƒí’ˆ ì¶”ì²œ ì„¹ì…˜ ì¶”ê°€",
                "5ë‹¨ê³„": "SEO ìµœì í™” (ë©”íƒ€ íƒœê·¸, í‚¤ì›Œë“œ ë°€ë„)",
                "6ë‹¨ê³„": "ê²Œì‹œ í›„ Google ê²€ìƒ‰ ë…¸ì¶œ ëŒ€ê¸°",
                "ğŸ’¡ ê¿€íŒ": "v3.2 ë‹¨ìˆœ ê²€ìƒ‰ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ê°œì¸ ê²½í—˜ë‹´"
            },
            
            "technical_details": {
                "countries_searched": countries_tried,
                "countries_available": list(TARGET_COUNTRIES.keys()),
                "processing_time": f"{processing_time:.2f}ì´ˆ",
                "generated_at": end_time.isoformat(),
                "api_version": "MoneyMaking_Crawler v3.2",
                "search_method": "ë‹¨ìˆœí™”ëœ í‚¤ì›Œë“œ ê²€ìƒ‰"
            }
        }
        
        print(f"\nğŸ‰ ë‹¨ìˆœí™”ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ’° ìµœì¢… ê²°ê³¼: {personal_article['word_count']}ë‹¨ì–´, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸ” ê²€ìƒ‰ ë°©ì‹: '{keyword}' í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ê²€ìƒ‰")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        
        return final_result
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "stage": "unknown",
            "api_version": "v3.2",
            "timestamp": datetime.utcnow().isoformat(),
            "troubleshooting": {
                "í™•ì¸ì‚¬í•­_1": "Google ì„œë¹„ìŠ¤ ê³„ì • ì„¤ì • í™•ì¸",
                "í™•ì¸ì‚¬í•­_2": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸",
                "í™•ì¸ì‚¬í•­_3": "í‚¤ì›Œë“œ ë° ìœ„ì¹˜ íŒŒë¼ë¯¸í„° í™•ì¸",
                "ê°œì„ ì‚¬í•­": "v3.2 ë‹¨ìˆœí™” ê²€ìƒ‰ ì‹œìŠ¤í…œ ì ìš©ë¨"
            }
        }

@app.route("/quick_test")
def quick_test():
    """ë¹ ë¥¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    try:
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë²ˆì—­ í…ŒìŠ¤íŠ¸
        test_keyword = "travel blog"
        test_translations = {}
        
        for country, info in list(TARGET_COUNTRIES.items())[:3]:  # 3ê°œêµ­ë§Œ í…ŒìŠ¤íŠ¸
            translated = translate_keyword(test_keyword, info['translate_to'])
            test_translations[country] = translated
        
        return {
            "success": True,
            "quick_test_results": {
                "ğŸ’° ì‹œìŠ¤í…œ ìƒíƒœ": "v3.2 ë‹¨ìˆœí™” ê²€ìƒ‰ ì •ìƒ ì‘ë™",
                "ğŸŒ í…ŒìŠ¤íŠ¸ ë²ˆì—­": test_translations,
                "ğŸ”§ Google ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if credentials else "âŒ ì—°ê²° ì•ˆë¨",
                "ğŸ“ Drive ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if drive_service else "âŒ ì—°ê²° ì•ˆë¨",
                "ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ ì°¨ë‹¨": f"{len(CORPORATE_EXCLUSIONS)}ê°œ ì‚¬ì´íŠ¸",
                "âœ… ê°œì¸ë¸”ë¡œê·¸ ê¸°ì¤€": "1ì  ì´ìƒ (ì™„í™”ë¨)",
                "ğŸ” ê²€ìƒ‰ ë°©ì‹": "í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ (ë‹¨ìˆœí™”)",
                "âš¡ ì²˜ë¦¬ ì†ë„": "ë¹ ë¦„ (ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ ì œê±°)",
                "ğŸ¯ ì¤€ë¹„ ìƒíƒœ": "ë‹¨ìˆœí™”ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ"
            },
            "improvements_v32": [
                "í‚¤ì›Œë“œ ë‹¨ìˆœí™” - êµ¬ê¸€ì‹œíŠ¸ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ê²€ìƒ‰",
                "ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì™„ì „ ì œê±°",
                "ê²€ìƒ‰ íšŸìˆ˜ ëŒ€í­ ê°ì†Œ (40ë²ˆâ†’10ë²ˆ)",
                "Google ì°¨ë‹¨ ìœ„í—˜ ìµœì†Œí™”",
                "ì²˜ë¦¬ ì†ë„ ë° ì•ˆì •ì„± í–¥ìƒ"
            ],
            "next_step": "global_crawl ì—”ë“œí¬ì¸íŠ¸ë¡œ ë‹¨ìˆœí™”ëœ í¬ë¡¤ë§ ì‹œì‘",
            "platform": "Railway",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8000))
    app.run(host='0.0.0.0', port=port)
