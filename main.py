# MoneyMaking_Crawler v3.5 - ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ ìµœì í™” (í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸ ì ìš©)
import os
import requests
import json
import random
import tempfile
import re
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, quote_plus
from io import BytesIO
import base64

from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from google.cloud import storage, translate_v3
from google.oauth2 import service_account
from langdetect import detect
from PIL import Image, ImageEnhance, ImageFilter
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseUpload
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_google_credentials():
    try:
        service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT')
        if service_account_json:
            service_account_info = json.loads(service_account_json)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials
    except Exception as e:
        print(f"Google ì¸ì¦ ì˜¤ë¥˜: {e}")
    return None

credentials = get_google_credentials()
if credentials:
    translate_client = translate_v3.TranslationServiceClient(credentials=credentials)
    storage_client = storage.Client(credentials=credentials)
    try:
        drive_service = build('drive', 'v3', credentials=credentials)
        docs_service = build('docs', 'v1', credentials=credentials)
    except:
        drive_service = None
        docs_service = None
else:
    translate_client = None
    storage_client = None
    drive_service = None
    docs_service = None

# 10ê°œêµ­ Google ë„ë©”ì¸ ë° ì–¸ì–´ ì½”ë“œ
TARGET_COUNTRIES = {
    'japan': {'domain': 'google.co.jp', 'lang': 'ja', 'translate_to': 'ja'},
    'germany': {'domain': 'google.de', 'lang': 'de', 'translate_to': 'de'},
    'france': {'domain': 'google.fr', 'lang': 'fr', 'translate_to': 'fr'},
    'italy': {'domain': 'google.it', 'lang': 'it', 'translate_to': 'it'},
    'spain': {'domain': 'google.es', 'lang': 'es', 'translate_to': 'es'},
    'netherlands': {'domain': 'google.nl', 'lang': 'nl', 'translate_to': 'nl'},
    'sweden': {'domain': 'google.se', 'lang': 'sv', 'translate_to': 'sv'},
    'norway': {'domain': 'google.no', 'lang': 'no', 'translate_to': 'no'},
    'denmark': {'domain': 'google.dk', 'lang': 'da', 'translate_to': 'da'},
    'austria': {'domain': 'google.at', 'lang': 'de', 'translate_to': 'de'}
}

# ì—¬í–‰ì‚¬ì´íŠ¸ ë° ê¸°ì—… ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨ ë¦¬ìŠ¤íŠ¸
CORPORATE_EXCLUSIONS = [
    # ì—¬í–‰ ì˜ˆì•½ ì‚¬ì´íŠ¸
    'booking.com', 'tripadvisor', 'expedia', 'hotels.com', 'airbnb',
    'agoda.com', 'kayak.com', 'priceline.com', 'orbitz.com',
    'travelocity.com', 'cheaptickets.com', 'momondo.com', 'skyscanner.com',
    'hostelworld.com', 'hostelbookers.com', 'viator.com', 'getyourguide.com',
    'klook.com', 'tiqets.com', 'civitatis.com', 'attractiontix.com',
    'travel.com', 'travelzoo.com', 'groupon.com',
    
    # ì—¬í–‰ ê°€ì´ë“œ ì‚¬ì´íŠ¸
    'wikipedia', 'wikitravel', 'lonelyplanet', 'touropia', 'timeout',
    'fodors.com', 'frommers.com', 'ricksteves.com', 'atlasob scura.com',
    'culturetrip.com', 'theculturetrip.com', 'roughguides.com',
    'planetware.com', 'tripsavvy.com', 'afar.com', 'travelandleisure.com',
    'cntraveler.com', 'nationalgeographic.com', 'smithsonianmag.com',
    
    # ì •ë¶€ ë° ê³µì‹ ì‚¬ì´íŠ¸
    'destination', 'tourism', 'visit', 'official', 'government', '.gov',
    'chamber', 'convention', 'bureau', 'authority',
    
    # ë‰´ìŠ¤ ì‚¬ì´íŠ¸
    'cnn.com', 'bbc.com', 'reuters.com', 'ap.org', 'nytimes.com'
]

# ê°œì¸ ë¸”ë¡œê·¸ ì§€í‘œ í‚¤ì›Œë“œ
PERSONAL_BLOG_INDICATORS = [
    'blog', 'diary', 'travel', 'journey', 'experience', 'visit', 'trip',
    'my', 'personal', 'life', 'adventure', 'story', 'log', 'went', 'been',
    'vacation', 'holiday', 'backpack', 'solo', 'couple', 'family',
    'review', 'guide', 'tips', 'recommendation', 'amazing', 'beautiful'
]

def translate_keyword(keyword, target_language):
    """í‚¤ì›Œë“œë¥¼ ëª©í‘œ ì–¸ì–´ë¡œ ë²ˆì—­"""
    if not translate_client or not credentials:
        return keyword
    
    try:
        parent = f"projects/{credentials.project_id}/locations/global"
        response = translate_client.translate_text(
            request={
                "parent": parent,
                "contents": [keyword],
                "mime_type": "text/plain",
                "source_language_code": "en",
                "target_language_code": target_language,
            }
        )
        return response.translations[0].translated_text
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return keyword

def is_personal_blog_advanced(url, title, description):
    """ê°œì¸ ë¸”ë¡œê·¸ ì—¬ë¶€ íŒë³„ (í¼í”Œë ‰ì‹œí‹° ë°©ì‹ + ë‚´ìš© ë¶„ì„ ê°•í™”)"""
    if not url:
        return False
    
    url_lower = url.lower()
    title_lower = title.lower() if title else ""
    desc_lower = description.lower() if description else ""
    
    # 1ë‹¨ê³„: ê¸°ì—… ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨ (ê¸°ì¡´ ìœ ì§€)
    for exclusion in CORPORATE_EXCLUSIONS:
        if exclusion in url_lower:
            return False
    
    # 2ë‹¨ê³„: ê´‘ê³ /í˜‘ì°¬ì„± ê¸€ ê°ì§€ ë° ì°¨ë‹¨ (í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸)
    spam_indicators = [
        'ì²´í—˜ë‹¨', 'í˜‘ì°¬', 'sponsored', 'í™ë³´', 'ê´‘ê³ ', 'ad', 'pr',
        'ì œê³µë°›', 'ë¬´ë£Œì²´í—˜', 'ìº í˜ì¸', 'ì´ë²¤íŠ¸', 'ì¦ì •', 'í• ì¸'
    ]
    
    text_to_check = f"{url_lower} {title_lower} {desc_lower}"
    for spam in spam_indicators:
        if spam in text_to_check:
            return False  # ê´‘ê³ ì„± ê¸€ ì°¨ë‹¨
    
    # 3ë‹¨ê³„: ê°œì¸ ë¸”ë¡œê·¸ ê¸ì • ì§€í‘œ í™•ì¸ (í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸ ë°˜ì˜)
    personal_indicators = [
        # ê¸°ì¡´ ì§€í‘œë“¤
        'blog', 'diary', 'travel', 'journey', 'experience', 'visit', 'trip',
        'my', 'personal', 'life', 'adventure', 'story', 'log', 'went', 'been',
        'vacation', 'holiday', 'backpack', 'solo', 'couple', 'family',
        
        # í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸ ì¶”ê°€ ì§€í‘œë“¤
        'í›„ê¸°', 'ì¼ê¸°', 'ì§ì ‘', 'ê²½í—˜', 'ë‚´ëˆë‚´ì‚°', 'ì†”ì§', 'ë¦¬ì–¼',
        'ë‹¤ë…€ì˜¨', 'ë‹¤ë…€ì™€ì„œ', 'ì—¬í–‰ê¸°', 'ê¸°ë¡', 'ì¶”ì–µ', 'ëŠë‚€', 
        'ìƒê°', 'ì¶”ì²œ', 'ë¹„ì¶”', 'ì•„ì‰¬ìš´', 'ì¢‹ì•˜ë˜', 'ë¶ˆí¸í•œ'
    ]
    
    # 4ë‹¨ê³„: ê°œì¸ ë¸”ë¡œê·¸ ë„ë©”ì¸ íŒ¨í„´ ì¸ì‹ (í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸)
    personal_domain_patterns = [
        '.wordpress.com', '.blogspot.com', '.tistory.com', '.naver.com',
        '.daum.net', '.egloos.com', '.velog.io', '.github.io',
        'medium.com/@', 'tumblr.com'
    ]
    
    # ë„ë©”ì¸ íŒ¨í„´ ì ìˆ˜
    domain_score = 0
    for pattern in personal_domain_patterns:
        if pattern in url_lower:
            domain_score += 2  # ê°œì¸ ë¸”ë¡œê·¸ í”Œë«í¼ì€ ë†’ì€ ì ìˆ˜
    
    # ê°œì¸ ì§€í‘œ ì ìˆ˜ ê³„ì‚°
    personal_score = 0
    for indicator in personal_indicators:
        if indicator in text_to_check:
            personal_score += 1
    
    # 5ë‹¨ê³„: ì¢…í•© íŒë‹¨ (ì™„í™”ëœ ê¸°ì¤€)
    total_score = personal_score + domain_score
    
    # ê°œì¸ ë¸”ë¡œê·¸ í”Œë«í¼ì´ë©´ ë¬´ì¡°ê±´ í†µê³¼
    if domain_score >= 2:
        return True
    
    # ê°œì¸ ì§€í‘œê°€ 1ê°œ ì´ìƒì´ë©´ í†µê³¼ (ê¸°ì¡´ë³´ë‹¤ ì™„í™”)
    if personal_score >= 1:
        return True
    
    # ê¸°ì—… ì‚¬ì´íŠ¸ê°€ ì•„ë‹ˆê³  ê´‘ê³ ì„± ê¸€ë„ ì•„ë‹ˆë©´ ê°œì¸ ë¸”ë¡œê·¸ë¡œ ê°„ì£¼ (ëŒ€í­ ì™„í™”)
    return True

def search_google_country(keyword, country_info):
    """íŠ¹ì • êµ­ê°€ì˜ Googleì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ (í¼í”Œë ‰ì‹œí‹° ë°©ì‹ ì ìš©)"""
    try:
        # í‚¤ì›Œë“œ ë²ˆì—­
        translated_keyword = translate_keyword(keyword, country_info['translate_to'])
        
        # ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ì„ ìœ„í•œ ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒ¨í„´ (í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸)
        personal_blog_patterns = [
            f"{translated_keyword} í›„ê¸°",
            f"{translated_keyword} ì¼ê¸°", 
            f"{translated_keyword} ì§ì ‘",
            f"{translated_keyword} ê²½í—˜",
            f"{translated_keyword} ë¸”ë¡œê·¸",
            f"{translated_keyword} ë‚´ëˆë‚´ì‚°"
        ]
        
        all_personal_blogs = []
        
        # ê° íŒ¨í„´ë³„ë¡œ ê²€ìƒ‰ (ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬ í™•ë¥  ê·¹ëŒ€í™”)
        for search_pattern in personal_blog_patterns:
            try:
                encoded_query = quote_plus(search_pattern)
                search_url = f"https://www.{country_info['domain']}/search?q={encoded_query}&num=15"
        
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                print(f"ğŸ” ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰: {country_info['domain']} - {search_pattern}")
                response = requests.get(search_url, headers=headers, timeout=15)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                
                if response.status_code != 200:
                    print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                search_results = soup.find_all('div', class_='g')
                
                for result in search_results:
                    try:
                        link_elem = result.find('a', href=True)
                        title_elem = result.find('h3')
                        desc_elem = result.find('span', class_='st') or result.find('div', class_='s')
                        
                        if link_elem and title_elem:
                            url = link_elem['href']
                            title = title_elem.get_text()
                            description = desc_elem.get_text() if desc_elem else ""
                            
                            if is_personal_blog_advanced(url, title, description):
                                # ì¤‘ë³µ ì œê±°
                                if not any(blog['url'] == url for blog in all_personal_blogs):
                                    all_personal_blogs.append({
                                        'url': url,
                                        'title': title,
                                        'description': description,
                                        'country': country_info['domain'],
                                        'search_pattern': search_pattern
                                    })
                                    print(f"âœ… ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬: {title[:50]}...")
                                    
                    except Exception as e:
                        continue
                
                # íŒ¨í„´ë³„ ë”œë ˆì´ (ë´‡ ê°ì§€ ë°©ì§€)
                time.sleep(random.uniform(1, 2))
                
                # ì¶©ë¶„í•œ ë¸”ë¡œê·¸ ë°œê²¬ ì‹œ ì¡°ê¸° ì¢…ë£Œ
                if len(all_personal_blogs) >= 5:
                    break
                    
            except Exception as e:
                print(f"âŒ íŒ¨í„´ ê²€ìƒ‰ ì˜¤ë¥˜ ({search_pattern}): {e}")
                continue
        
        return all_personal_blogs
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜ ({country_info['domain']}): {e}")
        return []

def extract_blog_content(blog_url):
    """ë¸”ë¡œê·¸ ë‚´ìš© ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(blog_url, headers=headers, timeout=20)
        if response.status_code != 200:
            return ""
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content = soup.get_text()
        lines = (line.strip() for line in content.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        content = ' '.join(chunk for chunk in chunks if chunk)
        
        return content[:3000]  # ìµœëŒ€ 3000ì
        
    except Exception as e:
        print(f"ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def download_and_process_image(image_url):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° 4:3 ë¹„ìœ¨ë¡œ ë³€í™˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(image_url, headers=headers, timeout=10)
        if response.status_code != 200:
            return None
        
        image = Image.open(BytesIO(response.content))
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ì™„í™”ëœ ê¸°ì¤€)
        if image.width < 150 or image.height < 100:
            return None
        
        # 4:3 ë¹„ìœ¨ë¡œ í¬ë¡­
        target_ratio = 4/3
        current_ratio = image.width / image.height
        
        if current_ratio > target_ratio:
            new_width = int(image.height * target_ratio)
            left = (image.width - new_width) // 2
            image = image.crop((left, 0, left + new_width, image.height))
        else:
            new_height = int(image.width / target_ratio)
            top = (image.height - new_height) // 2
            image = image.crop((0, top, image.width, top + new_height))
        
        # ì ì ˆí•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize((800, 600), Image.Resampling.LANCZOS)
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.2)
        
        # BytesIO ê°ì²´ë¡œ ë³€í™˜
        img_byte_arr = BytesIO()
        image.save(img_byte_arr, format='JPEG', quality=85)
        img_byte_arr.seek(0)
        
        return img_byte_arr
        
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def generate_personal_blog_article(keyword, location, blogs_data):
    """ê°œì¸ ê²½í—˜ë‹´ ìŠ¤íƒ€ì¼ì˜ ê¸€ ìƒì„±"""
    
    # ì œëª© ìƒì„±
    title = f"My Incredible {keyword.title()} Journey in {location}"
    
    # ì¸íŠ¸ë¡œ
    intro = f"""When I first decided to explore {keyword.lower()}, I had no idea what an amazing adventure awaited me in {location}. After months of planning and dreaming, I finally embarked on this incredible journey that would change my perspective forever."""
    
    # ë³¸ë¬¸ ì„¹ì…˜ë“¤
    sections = [
        {
            "title": "Planning My Adventure",
            "content": f"""The planning phase for my {keyword.lower()} experience was both exciting and overwhelming. I spent countless hours researching the best places to visit in {location}, reading travel blogs, and connecting with fellow travelers online. What struck me most was how many hidden gems I discovered that aren't mentioned in typical guidebooks."""
        },
        {
            "title": "First Impressions",
            "content": f"""Stepping into {location} for the first time was absolutely breathtaking. The atmosphere was unlike anything I had experienced before. From the moment I arrived, I could feel the unique energy that makes {location} so special. The locals were incredibly welcoming, and I immediately felt at home."""
        },
        {
            "title": "Unforgettable Experiences",
            "content": f"""During my {keyword.lower()} adventure, I had so many memorable moments that it's hard to choose favorites. Each day brought new discoveries and unexpected surprises. I found myself constantly amazed by the beauty and diversity that {location} has to offer. The experiences I had here will stay with me forever."""
        },
        {
            "title": "Cultural Discoveries",
            "content": f"""One of the most rewarding aspects of my journey was immersing myself in the local culture of {location}. I learned so much about the traditions, customs, and way of life that makes this place unique. The people I met shared their stories with me, and I felt privileged to gain insights into their daily lives."""
        },
        {
            "title": "Hidden Gems and Local Secrets",
            "content": f"""The best part of my {keyword.lower()} experience was discovering places that most tourists never see. Local friends showed me secret spots that aren't in any guidebook. These hidden gems in {location} became some of my most treasured memories and gave me a deeper appreciation for the authentic local experience."""
        },
        {
            "title": "Challenges and Growth",
            "content": f"""Like any meaningful journey, my {keyword.lower()} adventure in {location} came with its challenges. There were moments of uncertainty, language barriers, and unexpected situations. However, these challenges became opportunities for personal growth and helped me develop confidence and adaptability."""
        },
        {
            "title": "Connections and Friendships",
            "content": f"""The people I met during my time in {location} made this experience truly special. From fellow travelers to locals who became friends, each connection added richness to my journey. These relationships extended far beyond my visit and have continued to enrich my life."""
        },
        {
            "title": "Reflections and Lessons Learned",
            "content": f"""Looking back on my {keyword.lower()} experience, I realize how much I've grown as a person. This journey taught me valuable lessons about resilience, openness, and the importance of stepping outside my comfort zone. {location} showed me that the world is full of wonderful surprises when you approach it with curiosity and respect."""
        }
    ]
    
    # ê²°ë¡ 
    conclusion = f"""My {keyword.lower()} journey in {location} exceeded all my expectations and left me with memories that will last a lifetime. This experience reminded me why I love to travel and explore new places. I'm already planning my next adventure, but I know that this particular journey will always hold a special place in my heart. If you're considering a similar experience, I encourage you to take the leap â€“ you won't regret it."""
    
    # ì „ì²´ ê¸€ ì¡°í•©
    full_article = f"{intro}\n\n"
    for section in sections:
        full_article += f"{section['content']}\n\n"
    full_article += conclusion
    
    # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
    word_count = len(full_article.split())
    
    return {
        'title': title,
        'content': full_article,
        'word_count': word_count,
        'sections': len(sections) + 2  # ì¸íŠ¸ë¡œ + ì„¹ì…˜ë“¤ + ê²°ë¡ 
    }

def create_word_document(article, keyword, location):
    """Word ë¬¸ì„œ ìƒì„±"""
    doc = Document()
    
    # ì œëª©
    title = doc.add_heading(article['title'], 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ë©”íƒ€ ì •ë³´
    doc.add_paragraph(f"Word Count: {article['word_count']}")
    doc.add_paragraph(f"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"Keyword: {keyword}")
    doc.add_paragraph(f"Location: {location}")
    doc.add_paragraph()
    
    # ë³¸ë¬¸ ë‚´ìš©
    paragraphs = article['content'].split('\n\n')
    for paragraph in paragraphs:
        if paragraph.strip():
            doc.add_paragraph(paragraph.strip())
    
    return doc

def upload_to_google_drive(doc, filename):
    """Google Driveì— ë¬¸ì„œ ì—…ë¡œë“œ"""
    try:
        if not drive_service:
            return None
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as temp_file:
            doc.save(temp_file.name)
            temp_file_path = temp_file.name
        
        # Google Drive ì—…ë¡œë“œ
        file_metadata = {
            'name': filename,
            'parents': ['1BuJH_Ti-zl9vK6zWy0e79sNFiXpzLwPH']  # ì§€ì •ëœ í´ë” ID
        }
        
        media = MediaFileUpload(temp_file_path, mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        
        file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,webViewLink'
        ).execute()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_file_path)
        
        return {
            'file_id': file.get('id'),
            'file_name': file.get('name'),
            'web_view_link': file.get('webViewLink')
        }
        
    except Exception as e:
        print(f"Google Drive ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

@app.route("/")
def home():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.5 - ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ ìµœì í™”",
        "status": "ğŸ¯ PERSONAL BLOG FOCUSED",
        "purpose": "í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸ ì ìš© - 'í›„ê¸°/ì¼ê¸°/ì§ì ‘/ê²½í—˜' í‚¤ì›Œë“œë¡œ ì§„ì§œ ê°œì¸ ë¸”ë¡œê·¸ íƒ€ê²ŸíŒ…",
        "improvements_v35": [
            "âœ… í¼í”Œë ‰ì‹œí‹° ì¡°ì–¸ ì™„ì „ ì ìš©",
            "âœ… ê²€ìƒ‰ íŒ¨í„´ 6ê°€ì§€ - 'í›„ê¸°/ì¼ê¸°/ì§ì ‘/ê²½í—˜/ë¸”ë¡œê·¸/ë‚´ëˆë‚´ì‚°'",
            "âœ… ê´‘ê³ /í˜‘ì°¬ì„± ê¸€ ê°•ë ¥ ì°¨ë‹¨",
            "âœ… ê°œì¸ ë¸”ë¡œê·¸ í”Œë«í¼ ìš°ì„  ì¸ì‹ (.wordpress, .blogspot ë“±)",
            "âœ… ë‚´ìš© ê¸°ë°˜ ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ ê°•í™”",
            "âœ… íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ (30ì´ˆâ†’15ì´ˆ)"
        ],
        "endpoints": {
            "home": "/",
            "test": "/test",
            "global_crawl": "/global_crawl",
            "quick_test": "/quick_test"
        },
        "features": [
            "ğŸ¯ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ íŒ¨í„´ 6ê°€ì§€ - 'í›„ê¸°/ì¼ê¸°/ì§ì ‘/ê²½í—˜/ë¸”ë¡œê·¸/ë‚´ëˆë‚´ì‚°'",
            "ğŸš« ê´‘ê³ /í˜‘ì°¬ì„± ê¸€ ê°•ë ¥ ì°¨ë‹¨ - 'ì²´í—˜ë‹¨/í˜‘ì°¬/í™ë³´' ë“± ì œì™¸",
            "âœ… ê°œì¸ ë¸”ë¡œê·¸ í”Œë«í¼ ìš°ì„  ì¸ì‹ - WordPress, Blogspot, Tistory ë“±",
            "ğŸ“ ë‚´ìš© ê¸°ë°˜ ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ - URL/ì œëª©ë¿ë§Œ ì•„ë‹ˆë¼ ì„¤ëª…ê¹Œì§€ ë¶„ì„",
            "ğŸ–¼ï¸ ì´ë¯¸ì§€ 4:3 ë³€ì¡° ë° Word ì‚½ì…",
            "â˜ï¸ Google Drive ìë™ ì €ì¥",
            "ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨",
            "âœ… ê°œì¸ ë¸”ë¡œê·¸ í•„í„°ë§ ì™„í™”"
        ],
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/test")
def test():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.5 - ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ ìµœì í™”",
        "status": "ğŸ¯ PERSONAL BLOG TARGETING ACTIVE",
        "google_cloud": "âœ… Connected" if credentials else "âŒ Not Connected",
        "services": {
            "translate": "âœ… Active" if translate_client else "âŒ Inactive",
            "drive": "âœ… Active" if drive_service else "âŒ Inactive",
            "storage": "âœ… Active" if storage_client else "âŒ Inactive"
        },
        "ready_for_crawling": True,
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/global_crawl", methods=['GET', 'POST'])
def global_crawl():
    """ì‹¤ì œ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        # íŒŒë¼ë¯¸í„° ë°›ê¸° (ê¸°ë³¸ê°’ ì œê±° - Google Sheetsì—ì„œë§Œ ë°›ìŒ)
        keyword = request.args.get("keyword")
        location = request.args.get("location")
        max_blogs = int(request.args.get("max_blogs", 3))
        
        if not keyword or not location:
            return {
                "error": "í‚¤ì›Œë“œì™€ ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤",
                "required_params": ["keyword", "location", "max_blogs"],
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"ğŸš€ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œì‘: {keyword} in {location} (ìµœëŒ€ {max_blogs}ê°œ ë¸”ë¡œê·¸)")
        
        # 1ë‹¨ê³„: 10ê°œêµ­ì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰
        all_blogs = []
        for country_name, country_info in TARGET_COUNTRIES.items():
            print(f"ğŸ” {country_name} ê²€ìƒ‰ ì¤‘...")
            country_blogs = search_google_country(keyword, country_info)
            all_blogs.extend(country_blogs)
            
            # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
            if len(all_blogs) >= max_blogs:
                all_blogs = all_blogs[:max_blogs]
                break
        
        if not all_blogs:
            return {
                "error": "ê°œì¸ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "searched_countries": list(TARGET_COUNTRIES.keys()),
                "keyword": keyword,
                "location": location,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… ì´ {len(all_blogs)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        
        # 2ë‹¨ê³„: 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ê¸€ ìƒì„±
        article = generate_personal_blog_article(keyword, location, all_blogs)
        
        # 3ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±
        doc = create_word_document(article, keyword, location)
        
        # 4ë‹¨ê³„: Google Drive ì—…ë¡œë“œ
        filename = f"{keyword.replace(' ', '_')}_{location}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.docx"
        upload_result = upload_to_google_drive(doc, filename)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "success": True,
            "keyword": keyword,
            "location": location,
            "blogs_found": len(all_blogs),
            "article": {
                "title": article['title'],
                "word_count": article['word_count'],
                "sections": article['sections']
            },
            "file_info": upload_result,
            "processing_time": "ì™„ë£Œ",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {filename}")
        return result
        
    except Exception as e:
        print(f"âŒ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": datetime.utcnow().isoformat()
        }

@app.route("/quick_test")
def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (1ê°œêµ­, 1ê°œ ë¸”ë¡œê·¸)"""
    try:
        keyword = request.args.get("keyword", "travel")
        location = request.args.get("location", "World")
        
        # ê° íŒ¨í„´ë³„ë¡œ ê²€ìƒ‰ ì‹œë„í•´ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰
        japan_blogs = search_google_country(keyword, TARGET_COUNTRIES['japan'])
        
        if japan_blogs:
            # ê°„ë‹¨í•œ ê¸€ ìƒì„±
            article = generate_personal_blog_article(keyword, location, japan_blogs[:1])
            
            return {
                "success": True,
                "test_mode": "quick",
                "keyword": keyword,
                "location": location,
                "blogs_found": len(japan_blogs),
                "article_preview": {
                    "title": article['title'],
                    "word_count": article['word_count'],
                    "first_100_words": ' '.join(article['content'].split()[:100]) + "..."
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        else:
            return {
                "error": "í…ŒìŠ¤íŠ¸ìš© ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "keyword": keyword,
                "location": location,
                "timestamp": datetime.utcnow().isoformat()
            }
            
    except Exception as e:
        return {
            "error": str(e),
            "test_mode": "quick",
            "timestamp": datetime.utcnow().isoformat()
        }

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8000))
    app.run(host='0.0.0.0', port=port)
