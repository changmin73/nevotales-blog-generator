# MoneyMaking_Crawler v3.0 - ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
import os
import requests
import json
import random
import tempfile
import re
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, quote_plus
from io import BytesIO
import base64

from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from google.cloud import storage, translate_v3
from google.oauth2 import service_account
from langdetect import detect
from PIL import Image, ImageEnhance, ImageFilter
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseUpload
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_google_credentials():
    try:
        service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT')
        if service_account_json:
            service_account_info = json.loads(service_account_json)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials
    except Exception as e:
        print(f"Google ì¸ì¦ ì˜¤ë¥˜: {e}")
    return None

credentials = get_google_credentials()
if credentials:
    translate_client = translate_v3.TranslationServiceClient(credentials=credentials)
    storage_client = storage.Client(credentials=credentials)
    try:
        drive_service = build('drive', 'v3', credentials=credentials)
        docs_service = build('docs', 'v1', credentials=credentials)
    except:
        drive_service = None
        docs_service = None
else:
    translate_client = None
    storage_client = None
    drive_service = None
    docs_service = None

# 10ê°œêµ­ Google ë„ë©”ì¸ ë° ì–¸ì–´ ì½”ë“œ
TARGET_COUNTRIES = {
    'japan': {'domain': 'google.co.jp', 'lang': 'ja', 'translate_to': 'ja'},
    'germany': {'domain': 'google.de', 'lang': 'de', 'translate_to': 'de'},
    'france': {'domain': 'google.fr', 'lang': 'fr', 'translate_to': 'fr'},
    'italy': {'domain': 'google.it', 'lang': 'it', 'translate_to': 'it'},
    'spain': {'domain': 'google.es', 'lang': 'es', 'translate_to': 'es'},
    'netherlands': {'domain': 'google.nl', 'lang': 'nl', 'translate_to': 'nl'},
    'sweden': {'domain': 'google.se', 'lang': 'sv', 'translate_to': 'sv'},
    'norway': {'domain': 'google.no', 'lang': 'no', 'translate_to': 'no'},
    'denmark': {'domain': 'google.dk', 'lang': 'da', 'translate_to': 'da'},
    'austria': {'domain': 'google.at', 'lang': 'de', 'translate_to': 'de'}
}

# ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ í‚¤ì›Œë“œ
PERSONAL_BLOG_INDICATORS = [
    'blog', 'diary', 'travel', 'journey', 'experience', 'visit', 'trip',
    'my', 'personal', 'life', 'adventure', 'story', 'log'
]

CORPORATE_EXCLUSIONS = [
    'booking.com', 'tripadvisor', 'expedia', 'hotels.com', 'airbnb',
    'wikipedia', 'wikitravel', 'lonelyplanet', 'touropia', 'timeout',
    'cnn.com', 'bbc.com', 'reuters.com', 'news', 'commercial'
]

def translate_keyword(keyword, target_lang):
    """í‚¤ì›Œë“œë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­"""
    if not translate_client or not credentials:
        return keyword
    
    try:
        parent = f"projects/{credentials.project_id}/locations/global"
        response = translate_client.translate_text(
            request={
                "parent": parent,
                "contents": [keyword],
                "mime_type": "text/plain",
                "source_language_code": "en",
                "target_language_code": target_lang,
            }
        )
        translated = response.translations[0].translated_text
        print(f"í‚¤ì›Œë“œ ë²ˆì—­: {keyword} â†’ {translated} ({target_lang})")
        return translated
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return keyword

def is_personal_blog(url, title, description):
    """ê°œì¸ ë¸”ë¡œê·¸ì¸ì§€ íŒë³„"""
    url_lower = url.lower()
    title_lower = title.lower() if title else ""
    desc_lower = description.lower() if description else ""
    
    # ê¸°ì—… ì‚¬ì´íŠ¸ ì œì™¸
    for exclusion in CORPORATE_EXCLUSIONS:
        if exclusion in url_lower:
            return False
    
    # ê°œì¸ ë¸”ë¡œê·¸ ì§€í‘œ í™•ì¸
    text_to_check = f"{url_lower} {title_lower} {desc_lower}"
    personal_score = sum(1 for indicator in PERSONAL_BLOG_INDICATORS if indicator in text_to_check)
    
    return personal_score >= 2

def search_google_country(keyword, country_info, max_results=10):
    """íŠ¹ì • êµ­ê°€ì˜ Googleì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    
    # í‚¤ì›Œë“œë¥¼ í•´ë‹¹ êµ­ê°€ ì–¸ì–´ë¡œ ë²ˆì—­
    translated_keyword = translate_keyword(keyword, country_info['translate_to'])
    
    # Google ê²€ìƒ‰ URL êµ¬ì„±
    search_query = f"{translated_keyword} blog personal travel experience"
    encoded_query = quote_plus(search_query)
    
    search_url = f"https://www.{country_info['domain']}/search?q={encoded_query}&num={max_results}&hl={country_info['lang']}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': f"{country_info['lang']},en;q=0.9",
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }
    
    try:
        print(f"ğŸ” {country_info['domain']}ì—ì„œ ê²€ìƒ‰ ì¤‘: {translated_keyword}")
        response = requests.get(search_url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        results = []
        
        # Google ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        for result in soup.find_all('div', class_='g')[:max_results]:
            try:
                link_elem = result.find('a')
                if not link_elem or not link_elem.get('href'):
                    continue
                
                url = link_elem['href']
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                
                title_elem = result.find('h3')
                title = title_elem.get_text() if title_elem else ""
                
                desc_elem = result.find('span', class_='aCOpRe')
                if not desc_elem:
                    desc_elem = result.find('div', class_='VwiC3b')
                description = desc_elem.get_text() if desc_elem else ""
                
                # ê°œì¸ ë¸”ë¡œê·¸ íŒë³„
                if is_personal_blog(url, title, description):
                    results.append({
                        'url': url,
                        'title': title,
                        'description': description,
                        'country': country_info['domain'],
                        'language': country_info['lang']
                    })
                    print(f"âœ… ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬: {title[:50]}...")
                
            except Exception as e:
                continue
        
        print(f"ğŸ“Š {country_info['domain']}: {len(results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        return results
        
    except Exception as e:
        print(f"âŒ {country_info['domain']} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def process_4_3_image(img_data, keyword, index):
    """ì´ë¯¸ì§€ë¥¼ 4:3 ë¹„ìœ¨ë¡œ ì²˜ë¦¬í•˜ê³  ë³€ì¡°"""
    try:
        with Image.open(BytesIO(img_data)) as pil_img:
            # RGB ë³€í™˜
            if pil_img.mode != 'RGB':
                pil_img = pil_img.convert('RGB')
            
            width, height = pil_img.size
            
            # 4:3 ë¹„ìœ¨ë¡œ í¬ë¡­
            target_ratio = 4/3
            current_ratio = width/height
            
            if current_ratio > target_ratio:
                # ê°€ë¡œê°€ ë” ê¸´ ê²½ìš° - ì¢Œìš° í¬ë¡­
                new_width = int(height * target_ratio)
                left = (width - new_width) // 2
                pil_img = pil_img.crop((left, 0, left + new_width, height))
            elif current_ratio < target_ratio:
                # ì„¸ë¡œê°€ ë” ê¸´ ê²½ìš° - ìƒí•˜ í¬ë¡­
                new_height = int(width / target_ratio)
                top = (height - new_height) // 2
                pil_img = pil_img.crop((0, top, width, top + new_height))
            
            # 800x600ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            pil_img = pil_img.resize((800, 600), Image.LANCZOS)
            
            # ì´ë¯¸ì§€ ë³€ì¡° (ì¶”ì  ë°©ì§€)
            # 1. ë°ê¸°/ëŒ€ë¹„/ìƒ‰ìƒ ì¡°ì •
            pil_img = ImageEnhance.Brightness(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Contrast(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Color(pil_img).enhance(random.uniform(0.9, 1.1))
            
            # 2. ì¢Œìš° ë°˜ì „ (50% í™•ë¥ )
            if random.choice([True, False]):
                pil_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)
            
            # 3. ì•½ê°„ì˜ ë¸”ëŸ¬ íš¨ê³¼ (25% í™•ë¥ )
            if random.random() < 0.25:
                pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=0.5))
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_path = os.path.join(tempfile.gettempdir(), f"{keyword}_{index}_{random.randint(1000,9999)}.jpg")
            pil_img.save(temp_path, 'JPEG', quality=random.randint(85, 95), optimize=True)
            
            return temp_path
            
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def extract_blog_content(url, language):
    """ë¸”ë¡œê·¸ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_elements = soup.find_all(['p', 'div', 'article', 'section', 'main'])
        text_content = " ".join([elem.get_text(strip=True) for elem in text_elements])
        
        if len(text_content) < 500:  # ë„ˆë¬´ ì§§ì€ ê¸€ ì œì™¸
            return None
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = []
        img_tags = soup.find_all('img')
        
        for i, img in enumerate(img_tags[:20]):  # ìµœëŒ€ 20ê°œê¹Œì§€
            try:
                img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                if not img_url:
                    continue
                
                # URL ì •ê·œí™”
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    img_url = urljoin(url, img_url)
                elif not img_url.startswith('http'):
                    img_url = urljoin(url, img_url)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                img_response = requests.get(img_url, headers=headers, timeout=10)
                if img_response.status_code == 200:
                    img_data = img_response.content
                    
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    try:
                        with Image.open(BytesIO(img_data)) as test_img:
                            width, height = test_img.size
                            if width >= 200 and height >= 150:  # ìµœì†Œ í¬ê¸° ì²´í¬
                                processed_path = process_4_3_image(img_data, "image", len(images))
                                if processed_path:
                                    images.append(processed_path)
                    except:
                        continue
                        
            except Exception as e:
                continue
        
        # í…ìŠ¤íŠ¸ ë²ˆì—­
        if language != 'en':
            try:
                parent = f"projects/{credentials.project_id}/locations/global"
                response = translate_client.translate_text(
                    request={
                        "parent": parent,
                        "contents": [text_content[:2000]],  # ë²ˆì—­ ì œí•œ
                        "mime_type": "text/plain",
                        "source_language_code": language,
                        "target_language_code": "en",
                    }
                )
                translated_text = response.translations[0].translated_text
            except:
                translated_text = text_content[:2000]
        else:
            translated_text = text_content[:2000]
        
        return {
            'url': url,
            'original_text': text_content[:2000],
            'translated_text': translated_text,
            'images': images,
            'language': language
        }
        
    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ì¶”ì¶œ ì˜¤ë¥˜ {url}: {e}")
        return None

def create_personal_story(blog_contents, keyword, location):
    """ì—¬ëŸ¬ ë¸”ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±"""
    
    if not blog_contents:
        return {"error": "ì²˜ë¦¬í•  ë¸”ë¡œê·¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    # ëª¨ë“  ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_texts = []
    for content in blog_contents:
        if content.get('translated_text'):
            all_texts.append(content['translated_text'])
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
    all_sentences = []
    for text in all_texts:
        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        all_sentences.extend(valid_sentences)
    
    random.shuffle(all_sentences)
    
    # ì£¼ì œë³„ ë¬¸ì¥ ë¶„ë¥˜
    intro_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['first', 'started', 'began', 'decided', 'planned'])]
    experience_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['amazing', 'beautiful', 'incredible', 'wonderful', 'stunning', 'fantastic'])]
    place_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['place', 'location', 'area', 'spot', 'destination'])]
    food_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['food', 'eat', 'restaurant', 'delicious', 'taste', 'meal'])]
    tip_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['tip', 'advice', 'recommend', 'suggest', 'important'])]
    
    # 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ êµ¬ì„±
    article = {
        "title": f"My Incredible {keyword.title()} Journey in {location}: A Complete Personal Experience Guide",
        
        "introduction": f"""
When I first planned my trip to {location} for {keyword}, I had high expectations, but nothing could have prepared me for the incredible journey that awaited me. After spending extensive time exploring this amazing destination, I'm excited to share my personal experience and insider tips that will help you make the most of your own {keyword} adventure in {location}.

This isn't just another generic travel guide - this is my authentic, first-hand account of {keyword} in {location}, filled with personal stories, unexpected discoveries, and practical advice that I wish I had known before my trip. Whether you're planning your first visit or looking to discover something new, my experience will give you the confidence and knowledge to create your own unforgettable memories.
        """,
        
        "sections": [],
        
        "conclusion": f"""
Looking back on my {keyword} journey in {location}, I can honestly say it was one of the most transformative travel experiences of my life. Every single day brought new discoveries, incredible moments, and memories that I'll treasure forever.

{location} exceeded all my expectations for {keyword}, and I'm already planning my return trip. The combination of stunning landscapes, rich culture, amazing food, and warm people created an experience that touched my heart in ways I never expected.

If you're considering {location} for your next {keyword} adventure, I can't recommend it highly enough. Just remember to bring an open mind, comfortable shoes, and most importantly - be ready for the adventure of a lifetime. Trust me, you won't regret it!
        """,
        
        "word_count": 0
    }
    
    # 10ê°œ ëŒ€í˜• ì„¹ì…˜ ìƒì„±
    sections_data = [
        {
            "title": f"Why I Chose {location} for My {keyword.title()} Adventure",
            "sentences": intro_sentences[:5] + experience_sentences[:3],
            "intro": f"Let me start by sharing why {location} became my dream destination for {keyword}."
        },
        {
            "title": f"Planning My {keyword.title()} Trip to {location}: What I Learned",
            "sentences": tip_sentences[:4] + place_sentences[:4],
            "intro": f"Planning my {location} trip taught me valuable lessons that I wish I'd known earlier."
        },
        {
            "title": f"First Impressions: Arriving in {location}",
            "sentences": experience_sentences[3:7] + place_sentences[4:7],
            "intro": f"The moment I stepped foot in {location}, I knew this trip would be special."
        },
        {
            "title": f"Top {keyword.title()} Experiences That Blew My Mind",
            "sentences": experience_sentences[7:12] + place_sentences[7:10],
            "intro": f"Some experiences in {location} were so incredible, they completely changed my perspective on {keyword}."
        },
        {
            "title": f"Hidden Gems I Discovered During My {location} Journey",
            "sentences": place_sentences[10:15] + experience_sentences[12:15],
            "intro": f"The best parts of my {location} adventure were the unexpected discoveries off the beaten path."
        },
        {
            "title": f"Food Adventures: My Culinary Journey in {location}",
            "sentences": food_sentences[:8] + experience_sentences[15:18],
            "intro": f"The food scene in {location} became an unexpected highlight of my {keyword} journey."
        },
        {
            "title": f"Challenges I Faced and How I Overcame Them",
            "sentences": tip_sentences[4:8] + experience_sentences[18:21],
            "intro": f"Not everything went perfectly during my {location} trip, but these challenges taught me valuable lessons."
        },
        {
            "title": f"Meeting Locals: The Heart of My {location} Experience",
            "sentences": experience_sentences[21:25] + place_sentences[15:18],
            "intro": f"The people I met in {location} made my {keyword} journey truly unforgettable."
        },
        {
            "title": f"Budget Breakdown: What My {location} {keyword.title()} Trip Actually Cost",
            "sentences": tip_sentences[8:12] + experience_sentences[25:28],
            "intro": f"Here's the honest breakdown of what I spent during my {location} adventure."
        },
        {
            "title": f"Essential Tips for Your Own {location} {keyword.title()} Adventure",
            "sentences": tip_sentences[12:16] + place_sentences[18:22],
            "intro": f"Based on my experience, here are the most important things you need to know for {location}."
        }
    ]
    
    total_words = len(article["introduction"].split()) + len(article["conclusion"].split())
    
    # ê° ì„¹ì…˜ êµ¬ì„±
    for section_data in sections_data:
        if section_data["sentences"]:
            content = section_data["intro"] + " "
            
            # ë¬¸ì¥ë“¤ì„ ê°œì¸ ê²½í—˜ìœ¼ë¡œ ë³€í™˜
            personal_sentences = []
            for sentence in section_data["sentences"][:6]:
                # 2ì¸ì¹­/3ì¸ì¹­ì„ 1ì¸ì¹­ìœ¼ë¡œ ë³€í™˜
                personal_sentence = sentence
                personal_sentence = re.sub(r'\byou\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\byour\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\btheir\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\bthey\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\btravelers?\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\bvisitors?\b', 'I', personal_sentence, flags=re.IGNORECASE)
                
                personal_sentences.append(personal_sentence)
            
            full_content = content + " ".join(personal_sentences)
            
            article["sections"].append({
                "title": section_data["title"],
                "content": full_content
            })
            
            total_words += len(full_content.split())
    
    article["word_count"] = total_words
    return article

def create_word_document(article, all_images, keyword, location):
    """Word ë¬¸ì„œ ìƒì„± (ê¸€ + ì´ë¯¸ì§€ë“¤)"""
    
    doc = Document()
    
    # ì œëª©
    title = doc.add_heading(article['title'], 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ë©”íƒ€ ì •ë³´
    doc.add_paragraph(f"Word Count: {article['word_count']}")
    doc.add_paragraph(f"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"Keyword: {keyword}")
    doc.add_paragraph(f"Location: {location}")
    doc.add_paragraph()
    
    # ë„ì…ë¶€
    doc.add_heading("Introduction", 1)
    doc.add_paragraph(article['introduction'])
    
    # ë³¸ë¬¸ ì„¹ì…˜ë“¤
    for section in article['sections']:
        doc.add_heading(section['title'], 1)
        doc.add_paragraph(section['content'])
        doc.add_paragraph()  # ì„¹ì…˜ ê°„ ê³µë°±
    
    # ê²°ë¡ 
    doc.add_heading("Conclusion", 1)
    doc.add_paragraph(article['conclusion'])
    
    # ì´ë¯¸ì§€ ì„¹ì…˜
    if all_images:
        doc.add_page_break()
        doc.add_heading("ğŸ“¸ Collected Images (4:3 Ratio)", 1)
        doc.add_paragraph("All images have been processed and optimized for your blog:")
        doc.add_paragraph()
        
        # ì´ë¯¸ì§€ë“¤ ì‚½ì…
        for i, img_path in enumerate(all_images, 1):
            try:
                if os.path.exists(img_path):
                    doc.add_paragraph(f"Image {i}:")
                    doc.add_picture(img_path, width=Inches(5.33))  # 4:3 ë¹„ìœ¨ ìœ ì§€ (5.33" x 4")
                    doc.add_paragraph()
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ì‚½ì… ì˜¤ë¥˜ {img_path}: {e}")
                continue
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_docx_path = os.path.join(tempfile.gettempdir(), f"MoneyMaking_{keyword}_{location}_{int(time.time())}.docx")
    doc.save(temp_docx_path)
    
    return temp_docx_path

def upload_to_google_drive(docx_path, keyword, location):
    """Google Driveì— Word ë¬¸ì„œ ì—…ë¡œë“œ"""
    
    if not drive_service:
        return {"success": False, "error": "Google Drive ì„œë¹„ìŠ¤ ì—†ìŒ"}
    
    try:
        # ë©”ì¸ í´ë” ìƒì„±
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        folder_name = f"MoneyMaking_{keyword}_{location}_{timestamp}"
        
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        
        folder = drive_service.files().create(body=folder_metadata).execute()
        folder_id = folder.get('id')
        
        # Word ë¬¸ì„œ ì—…ë¡œë“œ
        file_metadata = {
            'name': f"MoneyMaking_{keyword}_{location}_Complete_Article.docx",
            'parents': [folder_id]
        }
        
        media = MediaFileUpload(docx_path, mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        
        uploaded_file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,webViewLink'
        ).execute()
        
        folder_link = f"https://drive.google.com/drive/folders/{folder_id}"
        
        return {
            "success": True,
            "folder_link": folder_link,
            "document_link": uploaded_file.get('webViewLink'),
            "folder_name": folder_name
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

# Flask ë¼ìš°íŠ¸ë“¤
@app.route("/")
def home():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.0 - ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ",
        "status": "ğŸš€ DEPLOYED ON RAILWAY",
        "purpose": "10ê°œêµ­ ê°œì¸ ë¸”ë¡œê·¸ í¬ë¡¤ë§ â†’ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„± â†’ Word ë¬¸ì„œ ìë™ ì €ì¥",
        "features": [
            "ğŸŒ 10ê°œêµ­ Google ê²€ìƒ‰ (ê°œì¸ ë¸”ë¡œê·¸ë§Œ íƒ€ê²ŸíŒ…)",
            "ğŸ” í‚¤ì›Œë“œ ìë™ ë²ˆì—­ (ê°êµ­ ì–¸ì–´)",
            "ğŸ“ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„±",
            "ğŸ–¼ï¸ ì´ë¯¸ì§€ 4:3 ë³€ì¡° ë° Word ì‚½ì…",
            "â˜ï¸ Google Drive ìë™ ì €ì¥"
        ],
        "endpoints": {
            "home": "/",
            "test": "/test",
            "global_crawl": "/global_crawl",
            "quick_test": "/quick_test"
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/test")
def test():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.0 System Check",
        "google_cloud": "âœ… Connected" if credentials else "âŒ Not Connected",
        "google_drive": "âœ… Connected" if drive_service else "âŒ Not Connected",
        "translate_service": "âœ… Connected" if translate_client else "âŒ Not Connected",
        "target_countries": len(TARGET_COUNTRIES),
        "status": "ğŸš€ READY FOR GLOBAL CRAWLING",
        "platform": "Railway",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/global_crawl", methods=['GET', 'POST'])
def global_crawl():
    """ğŸ’° ê¸€ë¡œë²Œ í¬ë¡¤ë§ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    try:
        # íŒŒë¼ë¯¸í„° ë°›ê¸°
        if request.method == 'GET':
            keyword = request.args.get("keyword", "travel")
            location = request.args.get("location", "Europe")
        else:
            data = request.get_json() or {}
            keyword = data.get("keyword", "travel")
            location = data.get("location", "Europe")
        
        start_time = datetime.utcnow()
        print(f"ğŸ’° ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œì‘: {keyword} in {location}")
        
        # 1ë‹¨ê³„: 10ê°œêµ­ì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰
        print("ğŸŒ 1ë‹¨ê³„: ê¸€ë¡œë²Œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰...")
        all_blog_results = []
        
        for country_name, country_info in TARGET_COUNTRIES.items():
            if len(all_blog_results) >= 10:  # 10ê°œ ìˆ˜ì§‘ë˜ë©´ ì¤‘ë‹¨
                break
                
            print(f"\nğŸ” {country_name} ê²€ìƒ‰ ì¤‘...")
            country_results = search_google_country(keyword, country_info, max_results=5)
            
            for result in country_results:
                if len(all_blog_results) >= 10:
                    break
                all_blog_results.append(result)
            
            time.sleep(2)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        if not all_blog_results:
            return {
                "success": False,
                "error": "ê°œì¸ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "blog_search",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(all_blog_results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        
        # 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© ë° ì´ë¯¸ì§€ í¬ë¡¤ë§
        print("\nğŸ“– 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§...")
        blog_contents = []
        all_images = []
        
        for i, blog_result in enumerate(all_blog_results):
            print(f"\ní¬ë¡¤ë§ ì¤‘ ({i+1}/{len(all_blog_results)}): {blog_result['title'][:50]}...")
            
            content = extract_blog_content(blog_result['url'], blog_result['language'])
            if content:
                blog_contents.append(content)
                all_images.extend(content['images'])
                print(f"âœ… ì„±ê³µ: {len(content['images'])}ê°œ ì´ë¯¸ì§€, {len(content['translated_text'])}ì")
            else:
                print(f"âŒ ì‹¤íŒ¨: {blog_result['url']}")
            
            time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        if not blog_contents:
            return {
                "success": False,
                "error": "ë¸”ë¡œê·¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "content_extraction",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(blog_contents)}ê°œ ë¸”ë¡œê·¸, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        
        # 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±
        print("\nâœï¸ 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ìƒì„±...")
        personal_article = create_personal_story(blog_contents, keyword, location)
        
        if "error" in personal_article:
            return {
                "success": False,
                "error": personal_article["error"],
                "stage": "story_creation",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {personal_article['word_count']}ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´")
        
        # 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±
        print("\nğŸ“„ 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±...")
        docx_path = create_word_document(personal_article, all_images, keyword, location)
        print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: Word ë¬¸ì„œ ìƒì„±")
        
        # 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ
        print("\nâ˜ï¸ 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ...")
        drive_result = upload_to_google_drive(docx_path, keyword, location)
        
        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        try:
            os.remove(docx_path)
            for img_path in all_images:
                if os.path.exists(img_path):
                    os.remove(img_path)
        except:
            pass
        
        # ìµœì¢… ê²°ê³¼
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        final_result = {
            "success": True,
            "global_crawling_info": {
                "ğŸ’° í‚¤ì›Œë“œ": keyword,
                "ğŸŒ íƒ€ê²Ÿ ìœ„ì¹˜": location,
                "â±ï¸ ì²˜ë¦¬ ì‹œê°„": f"{processing_time:.1f}ì´ˆ",
                "ğŸ” ê²€ìƒ‰ëœ ë¸”ë¡œê·¸": len(all_blog_results),
                "ğŸ“– ì„±ê³µì  í¬ë¡¤ë§": len(blog_contents),
                "ğŸ–¼ï¸ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€": len(all_images),
                "ğŸ“ ìµœì¢… ë‹¨ì–´ ìˆ˜": personal_article['word_count'],
                "ğŸ¯ ìƒíƒœ": "ì™„ë£Œ"
            },
            
            "blog_sources": [
                {
                    "title": result['title'],
                    "url": result['url'],
                    "country": result['country'],
                    "language": result['language']
                } for result in all_blog_results
            ],
            
            "article_preview": {
                "title": personal_article['title'],
                "word_count": personal_article['word_count'],
                "sections_count": len(personal_article['sections']),
                "introduction_preview": personal_article['introduction'][:200] + "...",
                "conclusion_preview": personal_article['conclusion'][:200] + "..."
            },
            
            "image_collection": {
                "total_processed": len(all_images),
                "format": "4:3 ratio (800x600)",
                "modifications": ["Brightness/Contrast adjusted", "50% chance horizontal flip", "Metadata removed"],
                "ready_for_blog": "âœ… Yes"
            },
            
            "google_drive_delivery": drive_result,
            
            "monetization_guide": {
                "1ë‹¨ê³„": "Google Driveì—ì„œ ì™„ì„±ëœ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                "2ë‹¨ê³„": "WordPressì— ê¸€ ë‚´ìš© ë³µì‚¬",
                "3ë‹¨ê³„": "ë¬¸ì„œ í•˜ë‹¨ì˜ ì´ë¯¸ì§€ë“¤ì„ ê¸€ ì¤‘ê°„ì¤‘ê°„ ì‚½ì…",
                "4ë‹¨ê³„": "ì–´í•„ë¦¬ì—ì´íŠ¸ ë§í¬ ë° ìƒí’ˆ ì¶”ì²œ ì„¹ì…˜ ì¶”ê°€",
                "5ë‹¨ê³„": "SEO ìµœì í™” (ë©”íƒ€ íƒœê·¸, í‚¤ì›Œë“œ ë°€ë„)",
                "6ë‹¨ê³„": "ê²Œì‹œ í›„ Google ê²€ìƒ‰ ë…¸ì¶œ ëŒ€ê¸°",
                "ğŸ’¡ ê¿€íŒ": "ê°œì¸ ê²½í—˜ë‹´ ìŠ¤íƒ€ì¼ì´ë¼ ì‹ ë¢°ë„ ë†’ìŒ, ì–´í•„ë¦¬ì—ì´íŠ¸ ì „í™˜ìœ¨ ê·¹ëŒ€í™”"
            },
            
            "technical_details": {
                "countries_searched": list(TARGET_COUNTRIES.keys()),
                "translation_languages": [info['lang'] for info in TARGET_COUNTRIES.values()],
                "processing_time": f"{processing_time:.2f}ì´ˆ",
                "generated_at": end_time.isoformat(),
                "api_version": "MoneyMaking_Crawler v3.0"
            }
        }
        
        print(f"\nğŸ‰ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ’° ìµœì¢… ê²°ê³¼: {personal_article['word_count']}ë‹¨ì–´, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        
        return final_result
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "stage": "unknown",
            "timestamp": datetime.utcnow().isoformat(),
            "troubleshooting": {
                "í™•ì¸ì‚¬í•­_1": "Google ì„œë¹„ìŠ¤ ê³„ì • ì„¤ì • í™•ì¸",
                "í™•ì¸ì‚¬í•­_2": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸",
                "í™•ì¸ì‚¬í•­_3": "í‚¤ì›Œë“œ ë° ìœ„ì¹˜ íŒŒë¼ë¯¸í„° í™•ì¸"
            }
        }

@app.route("/quick_test")
def quick_test():
    """ë¹ ë¥¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    try:
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë²ˆì—­ í…ŒìŠ¤íŠ¸
        test_keyword = "travel experience"
        test_translations = {}
        
        for country, info in list(TARGET_COUNTRIES.items())[:3]:  # 3ê°œêµ­ë§Œ í…ŒìŠ¤íŠ¸
            translated = translate_keyword(test_keyword, info['translate_to'])
            test_translations[country] = translated
        
        return {
            "success": True,
            "quick_test_results": {
                "ğŸ’° ì‹œìŠ¤í…œ ìƒíƒœ": "ì •ìƒ ì‘ë™",
                "ğŸŒ í…ŒìŠ¤íŠ¸ ë²ˆì—­": test_translations,
                "ğŸ”§ Google ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if credentials else "âŒ ì—°ê²° ì•ˆë¨",
                "ğŸ“ Drive ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if drive_service else "âŒ ì—°ê²° ì•ˆë¨",
                "âš¡ ì²˜ë¦¬ ì†ë„": "ë¹ ë¦„",
                "ğŸ¯ ì¤€ë¹„ ìƒíƒœ": "ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ"
            },
            "next_step": "global_crawl ì—”ë“œí¬ì¸íŠ¸ë¡œ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘",
            "platform": "Railway",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8000))
    app.run(host='0.0.0.0', port=port)
