# MoneyMaking_Crawler v3.1 - ê°œì„ ëœ í•„í„°ë§ ì‹œìŠ¤í…œ
import os
import requests
import json
import random
import tempfile
import re
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin, quote_plus
from io import BytesIO
import base64

from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from google.cloud import storage, translate_v3
from google.oauth2 import service_account
from langdetect import detect
from PIL import Image, ImageEnhance, ImageFilter
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseUpload
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_google_credentials():
    try:
        service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT')
        if service_account_json:
            service_account_info = json.loads(service_account_json)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            return credentials
    except Exception as e:
        print(f"Google ì¸ì¦ ì˜¤ë¥˜: {e}")
    return None

credentials = get_google_credentials()
if credentials:
    translate_client = translate_v3.TranslationServiceClient(credentials=credentials)
    storage_client = storage.Client(credentials=credentials)
    try:
        drive_service = build('drive', 'v3', credentials=credentials)
        docs_service = build('docs', 'v1', credentials=credentials)
    except:
        drive_service = None
        docs_service = None
else:
    translate_client = None
    storage_client = None
    drive_service = None
    docs_service = None

# 10ê°œêµ­ Google ë„ë©”ì¸ ë° ì–¸ì–´ ì½”ë“œ
TARGET_COUNTRIES = {
    'japan': {'domain': 'google.co.jp', 'lang': 'ja', 'translate_to': 'ja'},
    'germany': {'domain': 'google.de', 'lang': 'de', 'translate_to': 'de'},
    'france': {'domain': 'google.fr', 'lang': 'fr', 'translate_to': 'fr'},
    'italy': {'domain': 'google.it', 'lang': 'it', 'translate_to': 'it'},
    'spain': {'domain': 'google.es', 'lang': 'es', 'translate_to': 'es'},
    'netherlands': {'domain': 'google.nl', 'lang': 'nl', 'translate_to': 'nl'},
    'sweden': {'domain': 'google.se', 'lang': 'sv', 'translate_to': 'sv'},
    'norway': {'domain': 'google.no', 'lang': 'no', 'translate_to': 'no'},
    'denmark': {'domain': 'google.dk', 'lang': 'da', 'translate_to': 'da'},
    'austria': {'domain': 'google.at', 'lang': 'de', 'translate_to': 'de'}
}

# ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ í‚¤ì›Œë“œ (ëŒ€í­ í™•ì¥)
PERSONAL_BLOG_INDICATORS = [
    'blog', 'diary', 'travel', 'journey', 'experience', 'visit', 'trip',
    'my', 'personal', 'life', 'adventure', 'story', 'log', 'went', 'been',
    'vacation', 'holiday', 'backpack', 'solo', 'couple', 'family',
    'review', 'guide', 'tips', 'recommendation', 'amazing', 'beautiful',
    'incredible', 'awesome', 'wonderful', 'memories', 'discover', 'explore',
    'wandering', 'nomad', 'traveler', 'blogger', 'chronicles', 'tales',
    'adventures', 'experiences', 'memories', 'photos', 'pictures', 'moments'
]

# ê¸°ì—…/ì—¬í–‰ì‚¬ ì‚¬ì´íŠ¸ ì™„ì „ ì°¨ë‹¨ (ëŒ€í­ ê°•í™”)
CORPORATE_EXCLUSIONS = [
    # ì˜ˆì•½ ì‚¬ì´íŠ¸
    'booking.com', 'agoda.com', 'expedia.com', 'hotels.com', 'airbnb.com',
    'kayak.com', 'priceline.com', 'orbitz.com', 'travelocity.com',
    'cheaptickets.com', 'momondo.com', 'skyscanner.com', 'trivago.com',
    
    # í˜¸ìŠ¤í…”/ìˆ™ë°•
    'hostelworld.com', 'hostelbookers.com', 'hostelz.com', 'hostels.com',
    
    # íˆ¬ì–´/ì•¡í‹°ë¹„í‹°
    'viator.com', 'getyourguide.com', 'klook.com', 'tiqets.com',
    'civitatis.com', 'attractiontix.com', 'citypass.com', 'isango.com',
    
    # ì—¬í–‰ í¬í„¸/ë§¤ê±°ì§„
    'tripadvisor.com', 'lonelyplanet.com', 'roughguides.com', 'fodors.com',
    'frommers.com', 'ricksteves.com', 'atlasĞ¾Ğ±scura.com', 'culturetrip.com',
    'theculturetrip.com', 'planetware.com', 'tripsavvy.com', 'afar.com',
    'travelandleisure.com', 'cntraveler.com', 'timeout.com', 'touropia.com',
    
    # ë‰´ìŠ¤/ë§¤ê±°ì§„
    'cnn.com', 'bbc.com', 'reuters.com', 'nationalgeographic.com',
    'smithsonianmag.com', 'buzzfeed.com', 'huffpost.com',
    
    # ìœ„í‚¤/ì •ë³´
    'wikipedia.org', 'wikitravel.org', 'wikivoyage.org', 'wikimapia.org',
    
    # ì‡¼í•‘/ë”œ
    'groupon.com', 'livingsocial.com', 'travelzoo.com', 'travel.com',
    
    # ì •ë¶€/ê³µì‹
    'gov', 'government', 'official', 'tourism', 'visitkorea', 'visit',
    'destination', 'chamber', 'convention', 'bureau', 'authority',
    
    # ê¸°íƒ€ ê¸°ì—…
    'yelp.com', 'foursquare.com', 'facebook.com', 'instagram.com',
    'twitter.com', 'youtube.com', 'pinterest.com', 'reddit.com',
    
    # í‚¤ì›Œë“œ íŒ¨í„´
    'destination', 'tourism', 'visit', 'official', 'chamber', 'convention',
    'bureau', 'authority', 'commercial', 'advertisement', 'sponsored',
    'affiliate', 'deals', 'discount', 'cheap', 'best-price', 'compare-prices'
]

def translate_keyword(keyword, target_lang):
    """í‚¤ì›Œë“œë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­"""
    if not translate_client or not credentials:
        return keyword
    
    try:
        parent = f"projects/{credentials.project_id}/locations/global"
        response = translate_client.translate_text(
            request={
                "parent": parent,
                "contents": [keyword],
                "mime_type": "text/plain",
                "source_language_code": "en",
                "target_language_code": target_lang,
            }
        )
        translated = response.translations[0].translated_text
        print(f"í‚¤ì›Œë“œ ë²ˆì—­: {keyword} â†’ {translated} ({target_lang})")
        return translated
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return keyword

def is_personal_blog(url, title, description):
    """ê°œì¸ ë¸”ë¡œê·¸ì¸ì§€ íŒë³„ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)"""
    url_lower = url.lower()
    title_lower = title.lower() if title else ""
    desc_lower = description.lower() if description else ""
    
    # ê¸°ì—… ì‚¬ì´íŠ¸ ê°•ë ¥ ì œì™¸
    for exclusion in CORPORATE_EXCLUSIONS:
        if exclusion in url_lower:
            print(f"âŒ ê¸°ì—… ì‚¬ì´íŠ¸ ì œì™¸: {url} (í¬í•¨: {exclusion})")
            return False
    
    # ê°œì¸ ë¸”ë¡œê·¸ ì§€í‘œ í™•ì¸ (ê¸°ì¤€ ì™„í™”: 1ê°œë§Œ ìˆì–´ë„ OK)
    text_to_check = f"{url_lower} {title_lower} {desc_lower}"
    personal_score = sum(1 for indicator in PERSONAL_BLOG_INDICATORS if indicator in text_to_check)
    
    # ì¶”ê°€ ê°œì¸ ë¸”ë¡œê·¸ íŒ¨í„´
    personal_patterns = [
        'wordpress.com', 'blogspot.com', 'blogger.com', 'medium.com',
        'tumblr.com', 'ghost.io', 'substack.com', 'wix.com', 'squarespace.com',
        '/blog/', '/travel/', '/diary/', '/journal/', '/adventure/'
    ]
    
    pattern_score = sum(1 for pattern in personal_patterns if pattern in url_lower)
    
    total_score = personal_score + pattern_score
    is_personal = total_score >= 1  # ê¸°ì¤€ ëŒ€í­ ì™„í™”: 1ì ë§Œ ìˆì–´ë„ ê°œì¸ ë¸”ë¡œê·¸ë¡œ ì¸ì •
    
    print(f"{'âœ…' if is_personal else 'âŒ'} ë¸”ë¡œê·¸ íŒë³„: {url[:50]}... (ì ìˆ˜: {total_score})")
    return is_personal

def search_google_country(keyword, country_info, max_results=15):
    """íŠ¹ì • êµ­ê°€ì˜ Googleì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼)"""
    
    # í‚¤ì›Œë“œë¥¼ í•´ë‹¹ êµ­ê°€ ì–¸ì–´ë¡œ ë²ˆì—­
    translated_keyword = translate_keyword(keyword, country_info['translate_to'])
    
    # ë” ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹œë„
    search_queries = [
        f"{translated_keyword} blog personal experience",
        f"{translated_keyword} travel diary",
        f"{translated_keyword} my trip",
        f"{translated_keyword} adventure blog"
    ]
    
    all_results = []
    
    for search_query in search_queries:
        if len(all_results) >= 5:  # ê° ë‚˜ë¼ì—ì„œ ìµœëŒ€ 5ê°œ
            break
            
        encoded_query = quote_plus(search_query)
        search_url = f"https://www.{country_info['domain']}/search?q={encoded_query}&num={max_results}&hl={country_info['lang']}"
        
        # ë” ë‹¤ì–‘í•œ User-Agent ì‚¬ìš©
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept-Language': f"{country_info['lang']},en;q=0.9",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        try:
            print(f"ğŸ” {country_info['domain']}ì—ì„œ ê²€ìƒ‰: {search_query}")
            response = requests.get(search_url, headers=headers, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë” ë‹¤ì–‘í•œ Google ê²°ê³¼ íŒŒì‹± ì‹œë„
            search_results = soup.find_all('div', class_='g') or soup.find_all('div', class_='tF2Cxc')
            
            for result in search_results[:max_results]:
                if len(all_results) >= 5:
                    break
                    
                try:
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = result.find('a', href=True)
                    if not link_elem:
                        continue
                    
                    url = link_elem['href']
                    if url.startswith('/url?q='):
                        url = url.split('/url?q=')[1].split('&')[0]
                    elif url.startswith('/search'):
                        continue
                    
                    if not url.startswith('http'):
                        continue
                    
                    # ì œëª© ì¶”ì¶œ
                    title_elem = result.find('h3') or result.find('a')
                    title = title_elem.get_text() if title_elem else ""
                    
                    # ì„¤ëª… ì¶”ì¶œ
                    desc_candidates = result.find_all(['span', 'div'], class_=True)
                    description = ""
                    for candidate in desc_candidates:
                        text = candidate.get_text().strip()
                        if len(text) > 20 and len(text) < 200:
                            description = text
                            break
                    
                    # ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ (ì™„í™”ëœ ê¸°ì¤€)
                    if is_personal_blog(url, title, description):
                        all_results.append({
                            'url': url,
                            'title': title,
                            'description': description,
                            'country': country_info['domain'],
                            'language': country_info['lang'],
                            'search_query': search_query
                        })
                        print(f"âœ… ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬: {title[:50]}...")
                    
                except Exception as e:
                    print(f"ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            time.sleep(random.uniform(1, 3))  # ëœë¤ ë”œë ˆì´
            
        except Exception as e:
            print(f"âŒ {country_info['domain']} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue
    
    print(f"ğŸ“Š {country_info['domain']}: {len(all_results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
    return all_results

def process_4_3_image(img_data, keyword, index):
    """ì´ë¯¸ì§€ë¥¼ 4:3 ë¹„ìœ¨ë¡œ ì²˜ë¦¬í•˜ê³  ë³€ì¡°"""
    try:
        with Image.open(BytesIO(img_data)) as pil_img:
            # RGB ë³€í™˜
            if pil_img.mode != 'RGB':
                pil_img = pil_img.convert('RGB')
            
            width, height = pil_img.size
            
            # 4:3 ë¹„ìœ¨ë¡œ í¬ë¡­
            target_ratio = 4/3
            current_ratio = width/height
            
            if current_ratio > target_ratio:
                # ê°€ë¡œê°€ ë” ê¸´ ê²½ìš° - ì¢Œìš° í¬ë¡­
                new_width = int(height * target_ratio)
                left = (width - new_width) // 2
                pil_img = pil_img.crop((left, 0, left + new_width, height))
            elif current_ratio < target_ratio:
                # ì„¸ë¡œê°€ ë” ê¸´ ê²½ìš° - ìƒí•˜ í¬ë¡­
                new_height = int(width / target_ratio)
                top = (height - new_height) // 2
                pil_img = pil_img.crop((0, top, width, top + new_height))
            
            # 800x600ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            pil_img = pil_img.resize((800, 600), Image.LANCZOS)
            
            # ì´ë¯¸ì§€ ë³€ì¡° (ì¶”ì  ë°©ì§€)
            # 1. ë°ê¸°/ëŒ€ë¹„/ìƒ‰ìƒ ì¡°ì •
            pil_img = ImageEnhance.Brightness(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Contrast(pil_img).enhance(random.uniform(0.85, 1.15))
            pil_img = ImageEnhance.Color(pil_img).enhance(random.uniform(0.9, 1.1))
            
            # 2. ì¢Œìš° ë°˜ì „ (50% í™•ë¥ )
            if random.choice([True, False]):
                pil_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)
            
            # 3. ì•½ê°„ì˜ ë¸”ëŸ¬ íš¨ê³¼ (25% í™•ë¥ )
            if random.random() < 0.25:
                pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=0.5))
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_path = os.path.join(tempfile.gettempdir(), f"{keyword}_{index}_{random.randint(1000,9999)}.jpg")
            pil_img.save(temp_path, 'JPEG', quality=random.randint(85, 95), optimize=True)
            
            return temp_path
            
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def extract_blog_content(url, language):
    """ë¸”ë¡œê·¸ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_elements = soup.find_all(['p', 'div', 'article', 'section', 'main'])
        text_content = " ".join([elem.get_text(strip=True) for elem in text_elements])
        
        if len(text_content) < 300:  # ìµœì†Œ ê¸¸ì´ ê¸°ì¤€ ì™„í™”
            return None
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = []
        img_tags = soup.find_all('img')
        
        for i, img in enumerate(img_tags[:25]):  # ìµœëŒ€ 25ê°œê¹Œì§€
            try:
                img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                if not img_url:
                    continue
                
                # URL ì •ê·œí™”
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    img_url = urljoin(url, img_url)
                elif not img_url.startswith('http'):
                    img_url = urljoin(url, img_url)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                img_response = requests.get(img_url, headers=headers, timeout=10)
                if img_response.status_code == 200:
                    img_data = img_response.content
                    
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ê¸°ì¤€ ì™„í™”)
                    try:
                        with Image.open(BytesIO(img_data)) as test_img:
                            width, height = test_img.size
                            if width >= 150 and height >= 100:  # í¬ê¸° ê¸°ì¤€ ì™„í™”
                                processed_path = process_4_3_image(img_data, "image", len(images))
                                if processed_path:
                                    images.append(processed_path)
                    except:
                        continue
                        
            except Exception as e:
                continue
        
        # í…ìŠ¤íŠ¸ ë²ˆì—­
        if language != 'en':
            try:
                parent = f"projects/{credentials.project_id}/locations/global"
                response = translate_client.translate_text(
                    request={
                        "parent": parent,
                        "contents": [text_content[:2000]],  # ë²ˆì—­ ì œí•œ
                        "mime_type": "text/plain",
                        "source_language_code": language,
                        "target_language_code": "en",
                    }
                )
                translated_text = response.translations[0].translated_text
            except:
                translated_text = text_content[:2000]
        else:
            translated_text = text_content[:2000]
        
        return {
            'url': url,
            'original_text': text_content[:2000],
            'translated_text': translated_text,
            'images': images,
            'language': language
        }
        
    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ì¶”ì¶œ ì˜¤ë¥˜ {url}: {e}")
        return None

def create_personal_story(blog_contents, keyword, location):
    """ì—¬ëŸ¬ ë¸”ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±"""
    
    if not blog_contents:
        return {"error": "ì²˜ë¦¬í•  ë¸”ë¡œê·¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    # ëª¨ë“  ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_texts = []
    for content in blog_contents:
        if content.get('translated_text'):
            all_texts.append(content['translated_text'])
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
    all_sentences = []
    for text in all_texts:
        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        all_sentences.extend(valid_sentences)
    
    random.shuffle(all_sentences)
    
    # ì£¼ì œë³„ ë¬¸ì¥ ë¶„ë¥˜
    intro_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['first', 'started', 'began', 'decided', 'planned'])]
    experience_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['amazing', 'beautiful', 'incredible', 'wonderful', 'stunning', 'fantastic'])]
    place_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['place', 'location', 'area', 'spot', 'destination'])]
    food_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['food', 'eat', 'restaurant', 'delicious', 'taste', 'meal'])]
    tip_sentences = [s for s in all_sentences if any(word in s.lower() for word in ['tip', 'advice', 'recommend', 'suggest', 'important'])]
    
    # 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ êµ¬ì„±
    article = {
        "title": f"My Incredible {keyword.title()} Journey in {location}: A Complete Personal Experience Guide",
        
        "introduction": f"""
When I first planned my trip to {location} for {keyword}, I had high expectations, but nothing could have prepared me for the incredible journey that awaited me. After spending extensive time exploring this amazing destination, I'm excited to share my personal experience and insider tips that will help you make the most of your own {keyword} adventure in {location}.

This isn't just another generic travel guide - this is my authentic, first-hand account of {keyword} in {location}, filled with personal stories, unexpected discoveries, and practical advice that I wish I had known before my trip. Whether you're planning your first visit or looking to discover something new, my experience will give you the confidence and knowledge to create your own unforgettable memories.
        """,
        
        "sections": [],
        
        "conclusion": f"""
Looking back on my {keyword} journey in {location}, I can honestly say it was one of the most transformative travel experiences of my life. Every single day brought new discoveries, incredible moments, and memories that I'll treasure forever.

{location} exceeded all my expectations for {keyword}, and I'm already planning my return trip. The combination of stunning landscapes, rich culture, amazing food, and warm people created an experience that touched my heart in ways I never expected.

If you're considering {location} for your next {keyword} adventure, I can't recommend it highly enough. Just remember to bring an open mind, comfortable shoes, and most importantly - be ready for the adventure of a lifetime. Trust me, you won't regret it!
        """,
        
        "word_count": 0
    }
    
    # 10ê°œ ëŒ€í˜• ì„¹ì…˜ ìƒì„±
    sections_data = [
        {
            "title": f"Why I Chose {location} for My {keyword.title()} Adventure",
            "sentences": intro_sentences[:5] + experience_sentences[:3],
            "intro": f"Let me start by sharing why {location} became my dream destination for {keyword}."
        },
        {
            "title": f"Planning My {keyword.title()} Trip to {location}: What I Learned",
            "sentences": tip_sentences[:4] + place_sentences[:4],
            "intro": f"Planning my {location} trip taught me valuable lessons that I wish I'd known earlier."
        },
        {
            "title": f"First Impressions: Arriving in {location}",
            "sentences": experience_sentences[3:7] + place_sentences[4:7],
            "intro": f"The moment I stepped foot in {location}, I knew this trip would be special."
        },
        {
            "title": f"Top {keyword.title()} Experiences That Blew My Mind",
            "sentences": experience_sentences[7:12] + place_sentences[7:10],
            "intro": f"Some experiences in {location} were so incredible, they completely changed my perspective on {keyword}."
        },
        {
            "title": f"Hidden Gems I Discovered During My {location} Journey",
            "sentences": place_sentences[10:15] + experience_sentences[12:15],
            "intro": f"The best parts of my {location} adventure were the unexpected discoveries off the beaten path."
        },
        {
            "title": f"Food Adventures: My Culinary Journey in {location}",
            "sentences": food_sentences[:8] + experience_sentences[15:18],
            "intro": f"The food scene in {location} became an unexpected highlight of my {keyword} journey."
        },
        {
            "title": f"Challenges I Faced and How I Overcame Them",
            "sentences": tip_sentences[4:8] + experience_sentences[18:21],
            "intro": f"Not everything went perfectly during my {location} trip, but these challenges taught me valuable lessons."
        },
        {
            "title": f"Meeting Locals: The Heart of My {location} Experience",
            "sentences": experience_sentences[21:25] + place_sentences[15:18],
            "intro": f"The people I met in {location} made my {keyword} journey truly unforgettable."
        },
        {
            "title": f"Budget Breakdown: What My {location} {keyword.title()} Trip Actually Cost",
            "sentences": tip_sentences[8:12] + experience_sentences[25:28],
            "intro": f"Here's the honest breakdown of what I spent during my {location} adventure."
        },
        {
            "title": f"Essential Tips for Your Own {location} {keyword.title()} Adventure",
            "sentences": tip_sentences[12:16] + place_sentences[18:22],
            "intro": f"Based on my experience, here are the most important things you need to know for {location}."
        }
    ]
    
    total_words = len(article["introduction"].split()) + len(article["conclusion"].split())
    
    # ê° ì„¹ì…˜ êµ¬ì„±
    for section_data in sections_data:
        if section_data["sentences"]:
            content = section_data["intro"] + " "
            
            # ë¬¸ì¥ë“¤ì„ ê°œì¸ ê²½í—˜ìœ¼ë¡œ ë³€í™˜
            personal_sentences = []
            for sentence in section_data["sentences"][:6]:
                # 2ì¸ì¹­/3ì¸ì¹­ì„ 1ì¸ì¹­ìœ¼ë¡œ ë³€í™˜
                personal_sentence = sentence
                personal_sentence = re.sub(r'\byou\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\byour\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\btheir\b', 'my', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\bthey\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\btravelers?\b', 'I', personal_sentence, flags=re.IGNORECASE)
                personal_sentence = re.sub(r'\bvisitors?\b', 'I', personal_sentence, flags=re.IGNORECASE)
                
                personal_sentences.append(personal_sentence)
            
            full_content = content + " ".join(personal_sentences)
            
            article["sections"].append({
                "title": section_data["title"],
                "content": full_content
            })
            
            total_words += len(full_content.split())
    
    article["word_count"] = total_words
    return article

def create_word_document(article, all_images, keyword, location):
    """Word ë¬¸ì„œ ìƒì„± (ê¸€ + ì´ë¯¸ì§€ë“¤)"""
    
    doc = Document()
    
    # ì œëª©
    title = doc.add_heading(article['title'], 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ë©”íƒ€ ì •ë³´
    doc.add_paragraph(f"Word Count: {article['word_count']}")
    doc.add_paragraph(f"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"Keyword: {keyword}")
    doc.add_paragraph(f"Location: {location}")
    doc.add_paragraph()
    
    # ë„ì…ë¶€
    doc.add_heading("Introduction", 1)
    doc.add_paragraph(article['introduction'])
    
    # ë³¸ë¬¸ ì„¹ì…˜ë“¤
    for section in article['sections']:
        doc.add_heading(section['title'], 1)
        doc.add_paragraph(section['content'])
        doc.add_paragraph()  # ì„¹ì…˜ ê°„ ê³µë°±
    
    # ê²°ë¡ 
    doc.add_heading("Conclusion", 1)
    doc.add_paragraph(article['conclusion'])
    
    # ì´ë¯¸ì§€ ì„¹ì…˜
    if all_images:
        doc.add_page_break()
        doc.add_heading("ğŸ“¸ Collected Images (4:3 Ratio)", 1)
        doc.add_paragraph("All images have been processed and optimized for your blog:")
        doc.add_paragraph()
        
        # ì´ë¯¸ì§€ë“¤ ì‚½ì…
        for i, img_path in enumerate(all_images, 1):
            try:
                if os.path.exists(img_path):
                    doc.add_paragraph(f"Image {i}:")
                    doc.add_picture(img_path, width=Inches(5.33))  # 4:3 ë¹„ìœ¨ ìœ ì§€ (5.33" x 4")
                    doc.add_paragraph()
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ì‚½ì… ì˜¤ë¥˜ {img_path}: {e}")
                continue
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_docx_path = os.path.join(tempfile.gettempdir(), f"MoneyMaking_{keyword}_{location}_{int(time.time())}.docx")
    doc.save(temp_docx_path)
    
    return temp_docx_path

def upload_to_google_drive(docx_path, keyword, location):
    """Google Driveì— Word ë¬¸ì„œ ì—…ë¡œë“œ"""
    
    if not drive_service:
        return {"success": False, "error": "Google Drive ì„œë¹„ìŠ¤ ì—†ìŒ"}
    
    try:
        # ë©”ì¸ í´ë” ìƒì„±
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        folder_name = f"MoneyMaking_{keyword}_{location}_{timestamp}"
        
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        
        folder = drive_service.files().create(body=folder_metadata).execute()
        folder_id = folder.get('id')
        
        # Word ë¬¸ì„œ ì—…ë¡œë“œ
        file_metadata = {
            'name': f"MoneyMaking_{keyword}_{location}_Complete_Article.docx",
            'parents': [folder_id]
        }
        
        media = MediaFileUpload(docx_path, mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        
        uploaded_file = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,webViewLink'
        ).execute()
        
        folder_link = f"https://drive.google.com/drive/folders/{folder_id}"
        
        return {
            "success": True,
            "folder_link": folder_link,
            "document_link": uploaded_file.get('webViewLink'),
            "folder_name": folder_name
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

# Flask ë¼ìš°íŠ¸ë“¤
@app.route("/")
def home():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.1 - ê°œì„ ëœ í•„í„°ë§ ì‹œìŠ¤í…œ",
        "status": "ğŸš€ DEPLOYED ON RAILWAY",
        "purpose": "10ê°œêµ­ ê°œì¸ ë¸”ë¡œê·¸ í¬ë¡¤ë§ â†’ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„± â†’ Word ë¬¸ì„œ ìë™ ì €ì¥",
        "features": [
            "ğŸŒ 10ê°œêµ­ Google ê²€ìƒ‰ (ê°œì¸ ë¸”ë¡œê·¸ë§Œ íƒ€ê²ŸíŒ…)",
            "ğŸ” í‚¤ì›Œë“œ ìë™ ë²ˆì—­ (ê°êµ­ ì–¸ì–´)",
            "ğŸ“ 2500ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´ ìƒì„±",
            "ğŸ–¼ï¸ ì´ë¯¸ì§€ 4:3 ë³€ì¡° ë° Word ì‚½ì…",
            "â˜ï¸ Google Drive ìë™ ì €ì¥",
            "ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨",
            "âœ… ê°œì¸ ë¸”ë¡œê·¸ í•„í„°ë§ ì™„í™”"
        ],
        "improvements": [
            "ì—¬í–‰ì‚¬ì´íŠ¸ ì°¨ë‹¨ ëŒ€í­ ê°•í™” (50+ ì‚¬ì´íŠ¸)",
            "ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ ê¸°ì¤€ ì™„í™” (ì ìˆ˜ 2â†’1)",
            "ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ì–‘í™” (4ê°€ì§€ íŒ¨í„´)",
            "ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€ ì™„í™”",
            "User-Agent ë¡œí…Œì´ì…˜"
        ],
        "endpoints": {
            "home": "/",
            "test": "/test",
            "global_crawl": "/global_crawl",
            "quick_test": "/quick_test"
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/test")
def test():
    return {
        "message": "ğŸ’° MoneyMaking_Crawler v3.1 System Check",
        "google_cloud": "âœ… Connected" if credentials else "âŒ Not Connected",
        "google_drive": "âœ… Connected" if drive_service else "âŒ Not Connected",
        "translate_service": "âœ… Connected" if translate_client else "âŒ Not Connected",
        "target_countries": len(TARGET_COUNTRIES),
        "corporate_exclusions": len(CORPORATE_EXCLUSIONS),
        "personal_indicators": len(PERSONAL_BLOG_INDICATORS),
        "filtering_improvements": {
            "ê°œì¸ë¸”ë¡œê·¸_ê¸°ì¤€": "ì™„í™”ë¨ (1ì  ì´ìƒ)",
            "ì—¬í–‰ì‚¬ì´íŠ¸_ì°¨ë‹¨": f"{len(CORPORATE_EXCLUSIONS)}ê°œ ì‚¬ì´íŠ¸",
            "ê²€ìƒ‰_ì¿¼ë¦¬": "4ê°€ì§€ íŒ¨í„´",
            "User_Agent": "4ê°€ì§€ ë¡œí…Œì´ì…˜"
        },
        "status": "ğŸš€ READY FOR IMPROVED CRAWLING",
        "platform": "Railway",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.route("/global_crawl", methods=['GET', 'POST'])
def global_crawl():
    """ğŸ’° ê°œì„ ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    try:
        # íŒŒë¼ë¯¸í„° ë°›ê¸°
        if request.method == 'GET':
            keyword = request.args.get("keyword", "travel")
            location = request.args.get("location", "Europe")
            max_blogs = int(request.args.get("max_blogs", "3"))
        else:
            data = request.get_json() or {}
            keyword = data.get("keyword", "travel")
            location = data.get("location", "Europe")
            max_blogs = int(data.get("max_blogs", "3"))
        
        start_time = datetime.utcnow()
        print(f"ğŸ’° ê°œì„ ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì‹œì‘: {keyword} in {location}")
        
        # 1ë‹¨ê³„: 10ê°œêµ­ì—ì„œ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ê°œì„ ëœ í•„í„°ë§)
        print("ğŸŒ 1ë‹¨ê³„: ê°œì„ ëœ ê°œì¸ ë¸”ë¡œê·¸ ê²€ìƒ‰...")
        all_blog_results = []
        
        countries_tried = 0
        for country_name, country_info in TARGET_COUNTRIES.items():
            if len(all_blog_results) >= max_blogs:  # ëª©í‘œ ë‹¬ì„±í•˜ë©´ ì¤‘ë‹¨
                break
                
            countries_tried += 1
            print(f"\nğŸ” {country_name} ê²€ìƒ‰ ì¤‘ ({countries_tried}/10)...")
            country_results = search_google_country(keyword, country_info, max_results=20)
            
            for result in country_results:
                if len(all_blog_results) >= max_blogs:
                    break
                all_blog_results.append(result)
            
            print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(all_blog_results)}/{max_blogs}ê°œ")
            time.sleep(random.uniform(2, 4))  # ëœë¤ ë”œë ˆì´
        
        if not all_blog_results:
            return {
                "success": False,
                "error": "ê°œì„ ëœ í•„í„°ë§ìœ¼ë¡œë„ ê°œì¸ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "blog_search",
                "debug_info": {
                    "countries_tried": countries_tried,
                    "max_blogs_target": max_blogs,
                    "filtering_used": "v3.1 ê°œì„ ëœ í•„í„°ë§"
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(all_blog_results)}ê°œ ê°œì¸ ë¸”ë¡œê·¸ ë°œê²¬")
        
        # 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© ë° ì´ë¯¸ì§€ í¬ë¡¤ë§
        print("\nğŸ“– 2ë‹¨ê³„: ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§...")
        blog_contents = []
        all_images = []
        
        for i, blog_result in enumerate(all_blog_results):
            print(f"\ní¬ë¡¤ë§ ì¤‘ ({i+1}/{len(all_blog_results)}): {blog_result['title'][:50]}...")
            
            content = extract_blog_content(blog_result['url'], blog_result['language'])
            if content:
                blog_contents.append(content)
                all_images.extend(content['images'])
                print(f"âœ… ì„±ê³µ: {len(content['images'])}ê°œ ì´ë¯¸ì§€, {len(content['translated_text'])}ì")
            else:
                print(f"âŒ ì‹¤íŒ¨: {blog_result['url']}")
            
            time.sleep(random.uniform(1, 2))  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        if not blog_contents:
            return {
                "success": False,
                "error": "ë¸”ë¡œê·¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "stage": "content_extraction",
                "found_blogs": len(all_blog_results),
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(blog_contents)}ê°œ ë¸”ë¡œê·¸, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        
        # 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ì‘ì„±
        print("\nâœï¸ 3ë‹¨ê³„: ê°œì¸ ê²½í—˜ë‹´ ìƒì„±...")
        personal_article = create_personal_story(blog_contents, keyword, location)
        
        if "error" in personal_article:
            return {
                "success": False,
                "error": personal_article["error"],
                "stage": "story_creation",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {personal_article['word_count']}ë‹¨ì–´ ê°œì¸ ê²½í—˜ë‹´")
        
        # 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±
        print("\nğŸ“„ 4ë‹¨ê³„: Word ë¬¸ì„œ ìƒì„±...")
        docx_path = create_word_document(personal_article, all_images, keyword, location)
        print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: Word ë¬¸ì„œ ìƒì„±")
        
        # 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ
        print("\nâ˜ï¸ 5ë‹¨ê³„: Google Drive ì—…ë¡œë“œ...")
        drive_result = upload_to_google_drive(docx_path, keyword, location)
        
        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        try:
            os.remove(docx_path)
            for img_path in all_images:
                if os.path.exists(img_path):
                    os.remove(img_path)
        except:
            pass
        
        # ìµœì¢… ê²°ê³¼
        end_time = datetime.utcnow()
        processing_time = (end_time - start_time).total_seconds()
        
        final_result = {
            "success": True,
            "global_crawling_info": {
                "ğŸ’° í‚¤ì›Œë“œ": keyword,
                "ğŸŒ íƒ€ê²Ÿ ìœ„ì¹˜": location,
                "â±ï¸ ì²˜ë¦¬ ì‹œê°„": f"{processing_time:.1f}ì´ˆ",
                "ğŸ” ê²€ìƒ‰ëœ ë¸”ë¡œê·¸": len(all_blog_results),
                "ğŸ“– ì„±ê³µì  í¬ë¡¤ë§": len(blog_contents),
                "ğŸ–¼ï¸ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€": len(all_images),
                "ğŸ“ ìµœì¢… ë‹¨ì–´ ìˆ˜": personal_article['word_count'],
                "ğŸ¯ ìƒíƒœ": "v3.1 ê°œì„ ëœ í•„í„°ë§ìœ¼ë¡œ ì™„ë£Œ",
                "ğŸš« ì°¨ë‹¨ëœ ì—¬í–‰ì‚¬ì´íŠ¸": len(CORPORATE_EXCLUSIONS)
            },
            
            "filtering_improvements": {
                "ê°œì¸ë¸”ë¡œê·¸_ê¸°ì¤€": "1ì  ì´ìƒ (ì™„í™”ë¨)",
                "ì—¬í–‰ì‚¬ì´íŠ¸_ì°¨ë‹¨": f"{len(CORPORATE_EXCLUSIONS)}ê°œ ì‚¬ì´íŠ¸",
                "ê²€ìƒ‰_ë‹¤ì–‘í™”": "4ê°€ì§€ ì¿¼ë¦¬ íŒ¨í„´",
                "User_Agent_ë¡œí…Œì´ì…˜": "4ê°€ì§€",
                "ì´ë¯¸ì§€_ê¸°ì¤€_ì™„í™”": "150x100px â†’ ê´€ëŒ€í•œ ê¸°ì¤€"
            },
            
            "blog_sources": [
                {
                    "title": result['title'],
                    "url": result['url'],
                    "country": result['country'],
                    "language": result['language'],
                    "search_query": result.get('search_query', '')
                } for result in all_blog_results
            ],
            
            "article_preview": {
                "title": personal_article['title'],
                "word_count": personal_article['word_count'],
                "sections_count": len(personal_article['sections']),
                "introduction_preview": personal_article['introduction'][:200] + "...",
                "conclusion_preview": personal_article['conclusion'][:200] + "..."
            },
            
            "image_collection": {
                "total_processed": len(all_images),
                "format": "4:3 ratio (800x600)",
                "modifications": ["Brightness/Contrast adjusted", "50% chance horizontal flip", "Metadata removed"],
                "ready_for_blog": "âœ… Yes"
            },
            
            "google_drive_delivery": drive_result,
            
            "monetization_guide": {
                "1ë‹¨ê³„": "Google Driveì—ì„œ ì™„ì„±ëœ Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                "2ë‹¨ê³„": "WordPressì— ê¸€ ë‚´ìš© ë³µì‚¬",
                "3ë‹¨ê³„": "ë¬¸ì„œ í•˜ë‹¨ì˜ ì´ë¯¸ì§€ë“¤ì„ ê¸€ ì¤‘ê°„ì¤‘ê°„ ì‚½ì…",
                "4ë‹¨ê³„": "ì–´í•„ë¦¬ì—ì´íŠ¸ ë§í¬ ë° ìƒí’ˆ ì¶”ì²œ ì„¹ì…˜ ì¶”ê°€",
                "5ë‹¨ê³„": "SEO ìµœì í™” (ë©”íƒ€ íƒœê·¸, í‚¤ì›Œë“œ ë°€ë„)",
                "6ë‹¨ê³„": "ê²Œì‹œ í›„ Google ê²€ìƒ‰ ë…¸ì¶œ ëŒ€ê¸°",
                "ğŸ’¡ ê¿€íŒ": "v3.1 í•„í„°ë§ìœ¼ë¡œ ë” ìˆœìˆ˜í•œ ê°œì¸ ê²½í—˜ë‹´, ì‹ ë¢°ë„ ê·¹ëŒ€í™”"
            },
            
            "technical_details": {
                "countries_searched": countries_tried,
                "countries_available": list(TARGET_COUNTRIES.keys()),
                "processing_time": f"{processing_time:.2f}ì´ˆ",
                "generated_at": end_time.isoformat(),
                "api_version": "MoneyMaking_Crawler v3.1",
                "improvements": "í•„í„°ë§ ê°œì„ , ì—¬í–‰ì‚¬ì´íŠ¸ ì°¨ë‹¨ ê°•í™”"
            }
        }
        
        print(f"\nğŸ‰ ê°œì„ ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ’° ìµœì¢… ê²°ê³¼: {personal_article['word_count']}ë‹¨ì–´, {len(all_images)}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ {len(CORPORATE_EXCLUSIONS)}ê°œ ì°¨ë‹¨")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        
        return final_result
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "stage": "unknown",
            "api_version": "v3.1",
            "timestamp": datetime.utcnow().isoformat(),
            "troubleshooting": {
                "í™•ì¸ì‚¬í•­_1": "Google ì„œë¹„ìŠ¤ ê³„ì • ì„¤ì • í™•ì¸",
                "í™•ì¸ì‚¬í•­_2": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸",
                "í™•ì¸ì‚¬í•­_3": "í‚¤ì›Œë“œ ë° ìœ„ì¹˜ íŒŒë¼ë¯¸í„° í™•ì¸",
                "ê°œì„ ì‚¬í•­": "v3.1 í•„í„°ë§ ì‹œìŠ¤í…œ ì ìš©ë¨"
            }
        }

@app.route("/quick_test")
def quick_test():
    """ë¹ ë¥¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    try:
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë²ˆì—­ í…ŒìŠ¤íŠ¸
        test_keyword = "travel experience"
        test_translations = {}
        
        for country, info in list(TARGET_COUNTRIES.items())[:3]:  # 3ê°œêµ­ë§Œ í…ŒìŠ¤íŠ¸
            translated = translate_keyword(test_keyword, info['translate_to'])
            test_translations[country] = translated
        
        return {
            "success": True,
            "quick_test_results": {
                "ğŸ’° ì‹œìŠ¤í…œ ìƒíƒœ": "v3.1 ê°œì„ ëœ í•„í„°ë§ ì •ìƒ ì‘ë™",
                "ğŸŒ í…ŒìŠ¤íŠ¸ ë²ˆì—­": test_translations,
                "ğŸ”§ Google ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if credentials else "âŒ ì—°ê²° ì•ˆë¨",
                "ğŸ“ Drive ì„œë¹„ìŠ¤": "âœ… ì—°ê²°ë¨" if drive_service else "âŒ ì—°ê²° ì•ˆë¨",
                "ğŸš« ì—¬í–‰ì‚¬ì´íŠ¸ ì°¨ë‹¨": f"{len(CORPORATE_EXCLUSIONS)}ê°œ ì‚¬ì´íŠ¸",
                "âœ… ê°œì¸ë¸”ë¡œê·¸ ê¸°ì¤€": "1ì  ì´ìƒ (ì™„í™”ë¨)",
                "âš¡ ì²˜ë¦¬ ì†ë„": "ë¹ ë¦„",
                "ğŸ¯ ì¤€ë¹„ ìƒíƒœ": "ê°œì„ ëœ ê¸€ë¡œë²Œ í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ"
            },
            "improvements_v31": [
                "ì—¬í–‰ì‚¬ì´íŠ¸ ì°¨ë‹¨ 50+ ì‚¬ì´íŠ¸ë¡œ í™•ì¥",
                "ê°œì¸ ë¸”ë¡œê·¸ íŒë³„ ê¸°ì¤€ 2ì â†’1ì ìœ¼ë¡œ ì™„í™”",
                "ê²€ìƒ‰ ì¿¼ë¦¬ 4ê°€ì§€ íŒ¨í„´ìœ¼ë¡œ ë‹¤ì–‘í™”",
                "ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€ ì™„í™”",
                "User-Agent ë¡œí…Œì´ì…˜ 4ê°€ì§€"
            ],
            "next_step": "global_crawl ì—”ë“œí¬ì¸íŠ¸ë¡œ ê°œì„ ëœ í¬ë¡¤ë§ ì‹œì‘",
            "platform": "Railway",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8000))
    app.run(host='0.0.0.0', port=port)
